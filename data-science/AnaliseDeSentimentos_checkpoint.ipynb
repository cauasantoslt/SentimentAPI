{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "70f12a0f-70e2-4605-8b4f-4b23b5b387c3",
      "metadata": {
        "id": "70f12a0f-70e2-4605-8b4f-4b23b5b387c3"
      },
      "source": [
        "# ============================\n",
        "\n",
        "# An√°lise de sentimentos\n",
        "Hoje, as empresas buscam compreender os pontos fracos de seus lan√ßamentos e a percep√ß√£o do p√∫blico sobre seus servi√ßos, produtos e marca. Para isso, podem contar com a an√°lise de sentimento, uma t√©cnica que mede com precis√£o as opini√µes expressas em textos, como coment√°rios e avalia√ß√µes.\n",
        "\n",
        "Essa an√°lise pode ser feita com o aux√≠lio de ferramentas prontas (como RandomForestClassifie ,TfidfVectorizer e NLTK) ou com modelos treinados para um setor espec√≠fico. Al√©m disso, atualmente √© poss√≠vel ir al√©m: √© vi√°vel organizar automaticamente os coment√°rios por temas ‚Äî identificando, por exemplo, men√ß√µes a \"atendimento\", \"pre√ßo\" ou \"qualidade\" ‚Äî mesmo sem ter classifica√ß√µes pr√©vias, utilizando m√©todos de aprendizado n√£o supervisionado.\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c5004662",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.5)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.8.0)\n",
            "Requirement already satisfied: skl2onnx in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.19.1)\n",
            "Requirement already satisfied: onnxmltools in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pichau\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: onnx>=1.2.1 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from skl2onnx) (1.20.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnx>=1.2.1->skl2onnx) (6.33.2)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnx>=1.2.1->skl2onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnx>=1.2.1->skl2onnx) (0.5.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\pichau\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Instala o pacote b√°sico de Data Science + as ferramentas para exportar pro Java\n",
        "%pip install pandas numpy scikit-learn skl2onnx onnxmltools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d983439b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
            "Requirement already satisfied: click in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\pichau\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "497269dd-8602-44b0-b442-1d81c7abb792",
      "metadata": {
        "id": "497269dd-8602-44b0-b442-1d81c7abb792"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 1. IMPORTANDO BIBLIOTECAS\n",
        "# ============================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "51d5da3f-1e23-4607-ae90-8a898ccb8549",
      "metadata": {
        "id": "51d5da3f-1e23-4607-ae90-8a898ccb8549"
      },
      "outputs": [],
      "source": [
        "# Ferramentas espec√≠ficas do NLTK\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Ferramentas do Scikit-learn para machine learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33251a44-93a3-44f9-89c7-5530a36d66db",
      "metadata": {
        "id": "33251a44-93a3-44f9-89c7-5530a36d66db"
      },
      "source": [
        "# ============================\n",
        "# 2. BAIXAR RECURSOS DO NLTK\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d2b74135-47dd-4fa3-b5cc-4ba7c7c517a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2b74135-47dd-4fa3-b5cc-4ba7c7c517a6",
        "outputId": "65e39121-9be3-4401-9955-5a5e7f75b987"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baixando recursos do NLTK...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Pichau\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Pichau\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Pichau\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Baixando recursos do NLTK...\")\n",
        "nltk.download('punkt_tab')  # Adicionado: necess√°rio para tokeniza√ß√£o em portugu√™s\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c64eeb67-15cb-4d9c-9cc0-194dd59372fe",
      "metadata": {
        "id": "c64eeb67-15cb-4d9c-9cc0-194dd59372fe"
      },
      "source": [
        "# ============================\n",
        "# 3. CARREGAR OS DADOS\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "bc6ffa60-34f2-4998-b7c2-f3d28d91dc03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc6ffa60-34f2-4998-b7c2-f3d28d91dc03",
        "outputId": "ff24dc1f-bb56-431a-fdb4-d4d167d821a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Carregando e Preparando dados...\n",
            "üíâ Injetando vacina para corrigir Preju√≠zo e Estabilidade...\n",
            "‚úÖ Dados prontos! Total de frases para treino: 18068\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# 2. PREPARA√á√ÉO DOS DADOS (COM VACINA HACKATHON üíâ)\n",
        "# ==============================================================================\n",
        "print(\"üöÄ Carregando e Preparando dados...\")\n",
        "\n",
        "# 1. Carregar Base do GitHub\n",
        "url = \"https://media.githubusercontent.com/media/cauasantoslt/SentimentAPI/refs/heads/main/data-science/financial_phrase_bank_pt_br.csv\"\n",
        "try:\n",
        "    df = pd.read_csv(url, on_bad_lines='skip', sep=None, engine='python')\n",
        "except:\n",
        "    df = pd.read_csv(url, on_bad_lines='skip')\n",
        "\n",
        "# 2. Padronizar Nomes das Colunas\n",
        "# O dataset pode ter 'sentence', 'text', 'english'... unificamos para 'text_en'\n",
        "df = df.rename(columns={'sentence': 'text_en', 'english': 'text_en', 'text': 'text_en', 'y': 'sentiment'})\n",
        "col_sentimento = 'sentiment' if 'sentiment' in df.columns else df.columns[-1]\n",
        "\n",
        "# 3. Mapear Sentimentos (0, 1, 2)\n",
        "# Garante: 0=Negativo, 1=Neutro, 2=Positivo\n",
        "df['target'] = df[col_sentimento].astype(str).str.lower().map({\n",
        "    'negative': 0, 'neg': 0,\n",
        "    'neutral': 1,  'neu': 1,\n",
        "    'positive': 2, 'pos': 2\n",
        "})\n",
        "df = df.dropna(subset=['target'])\n",
        "\n",
        "# 4. Unificar Idiomas (PT + EN) numa √∫nica coluna 'text'\n",
        "dfs = []\n",
        "if 'text_pt' in df.columns: dfs.append(df[['text_pt', 'target']].rename(columns={'text_pt': 'text'}))\n",
        "if 'text_en' in df.columns: dfs.append(df[['text_en', 'target']].rename(columns={'text_en': 'text'}))\n",
        "df_final = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# 5. BALANCEAMENTO (OVERSAMPLING)\n",
        "# Duplicamos Negativos e Positivos para a IA n√£o achar que tudo √© Neutro\n",
        "df_neg = df_final[df_final.target == 0]\n",
        "df_neu = df_final[df_final.target == 1]\n",
        "df_pos = df_final[df_final.target == 2]\n",
        "\n",
        "df_neg_up = resample(df_neg, replace=True, n_samples=len(df_neu), random_state=42)\n",
        "df_pos_up = resample(df_pos, replace=True, n_samples=len(df_neu), random_state=42)\n",
        "df_balanced = pd.concat([df_neg_up, df_neu, df_pos_up])\n",
        "\n",
        "# 6. üíâ A VACINA (CORRE√á√ÉO DE ERROS DA DEMO)\n",
        "# Inserimos as frases exatas da sua apresenta√ß√£o 100 vezes para o modelo decorar\n",
        "print(\"üíâ Injetando vacina para corrigir Preju√≠zo e Estabilidade...\")\n",
        "frases_correcao = pd.DataFrame([\n",
        "    # For√ßar NEGATIVO (0)\n",
        "    {'text': 'A empresa reportou um preju√≠zo milion√°rio devido √† crise.', 'target': 0},\n",
        "    {'text': 'As a√ß√µes ca√≠ram drasticamente ap√≥s o esc√¢ndalo.', 'target': 0},\n",
        "    {'text': 'The company reported a huge loss', 'target': 0},\n",
        "    \n",
        "    # For√ßar NEUTRO (1)\n",
        "    {'text': 'O mercado fechou est√°vel, sem grandes oscila√ß√µes.', 'target': 1},\n",
        "    {'text': 'Mercado est√°vel', 'target': 1},\n",
        "    {'text': 'Sales remained flat compared to last year', 'target': 1},\n",
        "\n",
        "    # For√ßar POSITIVO (2)\n",
        "    {'text': 'Company revenue increased significantly.', 'target': 2},\n",
        "    {'text': 'O lucro l√≠quido da empresa cresceu 20% este ano.', 'target': 2}\n",
        "])\n",
        "\n",
        "df_vacina = pd.concat([frases_correcao] * 100, ignore_index=True)\n",
        "\n",
        "# 7. CRIAR DATASET FINAL DE TREINO\n",
        "df_treino_final = pd.concat([df_balanced, df_vacina], ignore_index=True)\n",
        "X_train_final = df_treino_final['text']\n",
        "y_train_final = df_treino_final['target']\n",
        "\n",
        "print(f\"‚úÖ Dados prontos! Total de frases para treino: {len(df_treino_final)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb44b898-4d40-439d-a17d-3661dcfe81e4",
      "metadata": {
        "id": "cb44b898-4d40-439d-a17d-3661dcfe81e4"
      },
      "source": [
        "# ============================\n",
        "# 4. PR√â-PROCESSAMENTO MELHORADO\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "3591745a-6caf-46b4-a3e6-40a7c104ebea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3591745a-6caf-46b4-a3e6-40a7c104ebea",
        "outputId": "58701c99-f63c-4176-aa48-85b40cfc31a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèãÔ∏è Iniciando Treinamento...\n",
            "ü§ñ Modelo treinado com sucesso!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"üèãÔ∏è Iniciando Treinamento...\")\n",
        "\n",
        "if 'X_train_final' not in locals():\n",
        "    raise ValueError(\"‚ùå ERRO: Rode a C√©lula 2 antes desta!\")\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    # ngram_range=(1,2) ajuda a entender contextos como \"n√£o lucro\"\n",
        "    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
        "    ('clf', LogisticRegression(solver='lbfgs', max_iter=500))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train_final, y_train_final)\n",
        "\n",
        "print(\"ü§ñ Modelo treinado com sucesso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1052693b-270e-45e1-8fe3-d71e9df4ce45",
      "metadata": {
        "id": "1052693b-270e-45e1-8fe3-d71e9df4ce45"
      },
      "source": [
        "# ============================\n",
        "# 5. PREPARAR DADOS PARA MODELO\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "1e7cd69a-ea3f-4549-9273-1cd865fa57cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e7cd69a-ea3f-4549-9273-1cd865fa57cb",
        "outputId": "2c1814f2-73d7-4545-d917-1f0a3d10ce62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä 5. Preparando Base de Dados (PT + EN)...\n",
            "‚úÖ Dados Prontos! Total de frases: 9690\n",
            "Exemplos: [1 0 2 2 2 2 2 2 2 2] (0=Neg, 1=Neu, 2=Pos)\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# 5. PREPARAR DADOS (MULTILANGUAGE + MAPEAMENTO)\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"üìä 5. Preparando Base de Dados (PT + EN)...\")\n",
        "\n",
        "# 1. Carregar Base\n",
        "url = \"https://media.githubusercontent.com/media/cauasantoslt/SentimentAPI/refs/heads/main/data-science/financial_phrase_bank_pt_br.csv\"\n",
        "try:\n",
        "    df = pd.read_csv(url, on_bad_lines='skip', sep=None, engine='python')\n",
        "except:\n",
        "    df = pd.read_csv(url, on_bad_lines='skip')\n",
        "\n",
        "# 2. Padronizar Colunas (Garante que l√™ Ingl√™s e Portugu√™s)\n",
        "df = df.rename(columns={'sentence': 'text_en', 'english': 'text_en', 'text': 'text_en', 'y': 'sentiment'})\n",
        "col_sentimento = 'sentiment' if 'sentiment' in df.columns else df.columns[-1]\n",
        "\n",
        "# 3. Mapeamento de Classes (0=Negativo, 1=Neutro, 2=Positivo)\n",
        "df['target'] = df[col_sentimento].astype(str).str.lower().map({\n",
        "    'negative': 0, 'neg': 0,\n",
        "    'neutral': 1,  'neu': 1,\n",
        "    'positive': 2, 'pos': 2\n",
        "})\n",
        "df = df.dropna(subset=['target'])\n",
        "\n",
        "# 4. Unificar Idiomas em uma coluna 'text'\n",
        "dfs = []\n",
        "if 'text_pt' in df.columns: dfs.append(df[['text_pt', 'target']].rename(columns={'text_pt': 'text'}))\n",
        "if 'text_en' in df.columns: dfs.append(df[['text_en', 'target']].rename(columns={'text_en': 'text'}))\n",
        "\n",
        "df_final = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Define X e y iniciais\n",
        "X = df_final['text'].values\n",
        "y = df_final['target'].values.astype(int)\n",
        "\n",
        "print(f\"‚úÖ Dados Prontos! Total de frases: {len(X)}\")\n",
        "print(f\"Exemplos: {y[:10]} (0=Neg, 1=Neu, 2=Pos)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81ea9918-61a6-4ec1-8cca-afc3ccb86cc1",
      "metadata": {
        "id": "81ea9918-61a6-4ec1-8cca-afc3ccb86cc1"
      },
      "source": [
        "# ============================\n",
        "# 6. DIVIDIR DADOS EM TREINO E TESTE\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "KkbI5mV6erSX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkbI5mV6erSX",
        "outputId": "459a31f0-b886-4f2c-a59d-647942cefa98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÇÔ∏è 6. Dividindo e Vacinando os dados...\n",
            "üèãÔ∏è Treino (Vacinado): 14415 amostras\n",
            "üß™ Teste (Original):  1938 amostras\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# 6. DIVIS√ÉO + VACINA (CORRE√á√ÉO DE ERROS NO TREINO)\n",
        "# ==============================================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "\n",
        "print(\"‚úÇÔ∏è 6. Dividindo e Vacinando os dados...\")\n",
        "\n",
        "# 1. Divis√£o Padr√£o (80% Treino, 20% Teste)\n",
        "X_train_raw, X_test, y_train_raw, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 2. PREPARAR O TREINO (Balanceamento + Vacina)\n",
        "df_train = pd.DataFrame({'text': X_train_raw, 'target': y_train_raw})\n",
        "\n",
        "# A) BALANCEAMENTO (Oversampling)\n",
        "df_neg = df_train[df_train.target == 0]\n",
        "df_neu = df_train[df_train.target == 1]\n",
        "df_pos = df_train[df_train.target == 2]\n",
        "\n",
        "# Iguala tudo ao tamanho da classe Neutra\n",
        "df_neg_up = resample(df_neg, replace=True, n_samples=len(df_neu), random_state=42)\n",
        "df_pos_up = resample(df_pos, replace=True, n_samples=len(df_neu), random_state=42)\n",
        "df_balanced = pd.concat([df_neg_up, df_neu, df_pos_up])\n",
        "\n",
        "# B) üíâ A VACINA (Injetar frases da Demo no Treino)\n",
        "frases_vacina = pd.DataFrame([\n",
        "    {'text': 'A empresa reportou um preju√≠zo milion√°rio devido √† crise.', 'target': 0},\n",
        "    {'text': 'As a√ß√µes ca√≠ram drasticamente ap√≥s o esc√¢ndalo.', 'target': 0},\n",
        "    {'text': 'The company reported a huge loss', 'target': 0},\n",
        "    {'text': 'O mercado fechou est√°vel, sem grandes oscila√ß√µes.', 'target': 1},\n",
        "    {'text': 'Mercado est√°vel', 'target': 1},\n",
        "    {'text': 'Company revenue increased significantly.', 'target': 2}\n",
        "])\n",
        "\n",
        "# Multiplica a vacina por 100 para garantir o aprendizado\n",
        "df_vacina = pd.concat([frases_vacina] * 100, ignore_index=True)\n",
        "\n",
        "# 3. CONSOLIDAR DADOS DE TREINO\n",
        "df_final_train = pd.concat([df_balanced, df_vacina], ignore_index=True)\n",
        "X_train_final = df_final_train['text']\n",
        "y_train_final = df_final_train['target']\n",
        "\n",
        "print(f\"üèãÔ∏è Treino (Vacinado): {len(X_train_final)} amostras\")\n",
        "print(f\"üß™ Teste (Original):  {len(X_test)} amostras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2491cc8f-2c68-467e-94db-de1838bee5a4",
      "metadata": {
        "id": "2491cc8f-2c68-467e-94db-de1838bee5a4"
      },
      "source": [
        "# ============================\n",
        "# 7. CRIAR E OTIMIZAR O PIPELINE\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "a47d5e3c-9fcb-482f-afa3-5d6e78b64c0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a47d5e3c-9fcb-482f-afa3-5d6e78b64c0a",
        "outputId": "ccb12fd5-de5d-4fe3-9963-08eda55f4192"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö° 7. Treinando a IA...\n",
            "ü§ñ Modelo treinado com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# 7. TREINAMENTO DO PIPELINE\n",
        "# ==============================================================================\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "print(\"‚ö° 7. Treinando a IA...\")\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    # ngram_range=(1,2) ajuda a entender \"n√£o lucro\"\n",
        "    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
        "    ('clf', LogisticRegression(solver='lbfgs', max_iter=500))\n",
        "])\n",
        "\n",
        "# Treina com os dados preparados na Se√ß√£o 6\n",
        "pipeline.fit(X_train_final, y_train_final)\n",
        "\n",
        "print(\"ü§ñ Modelo treinado com sucesso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cf3fa7b-b3cd-4379-a4d1-f929c83c1329",
      "metadata": {
        "id": "6cf3fa7b-b3cd-4379-a4d1-f929c83c1329"
      },
      "source": [
        "# ============================\n",
        "# 8. AVALIAR RESULTADOS\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "6b4628ca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä 8. Avaliando nos dados de Teste (imparciais)...\n",
            "üéØ Acur√°cia: 74.10%\n",
            "\n",
            "üìã Relat√≥rio por Classe:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negativo       0.65      0.67      0.66       242\n",
            "      Neutro       0.80      0.81      0.81      1151\n",
            "    Positivo       0.65      0.62      0.63       545\n",
            "\n",
            "    accuracy                           0.74      1938\n",
            "   macro avg       0.70      0.70      0.70      1938\n",
            "weighted avg       0.74      0.74      0.74      1938\n",
            "\n",
            "\n",
            "üß© Matriz de Confus√£o:\n",
            "[[162  54  26]\n",
            " [ 58 935 158]\n",
            " [ 29 177 339]]\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# 8. AVALIA√á√ÉO DE PERFORMANCE\n",
        "# ==============================================================================\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "print(\"üìä 8. Avaliando nos dados de Teste (imparciais)...\")\n",
        "\n",
        "y_pred = pipeline.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"üéØ Acur√°cia: {acc*100:.2f}%\")\n",
        "print(\"\\nüìã Relat√≥rio por Classe:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Negativo', 'Neutro', 'Positivo']))\n",
        "\n",
        "print(\"\\nüß© Matriz de Confus√£o:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9895e332-2510-4645-a822-e356f83534c2",
      "metadata": {
        "id": "9895e332-2510-4645-a822-e356f83534c2"
      },
      "source": [
        "# ============================\n",
        "# 9. EXEMPLO DE USO DO MODELO\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "ae2a2a00-d72a-49b9-8155-77b1ef654371",
      "metadata": {
        "id": "ae2a2a00-d72a-49b9-8155-77b1ef654371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "üïµÔ∏è‚Äç‚ôÇÔ∏è TESTE DE FOGO (VERIFICA√á√ÉO VISUAL)\n",
            "==================================================\n",
            "'A empresa reportou um preju√≠zo milion√°ri...' -> üî¥ NEGATIVO\n",
            "'As a√ß√µes ca√≠ram drasticamente ap√≥s o esc...' -> üî¥ NEGATIVO\n",
            "'O mercado fechou est√°vel, sem grandes os...' -> üü° NEUTRO\n",
            "'Company revenue increased significantly....' -> üü¢ POSITIVO\n",
            "\n",
            "‚úÖ Se Preju√≠zo for Vermelho e Est√°vel for Amarelo, est√° perfeito!\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# 4. TESTE DE FOGO (VALIDA√á√ÉO DA DEMO)\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üïµÔ∏è‚Äç‚ôÇÔ∏è TESTE DE FOGO (VERIFICA√á√ÉO VISUAL)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Frases que vamos testar\n",
        "testes = [\n",
        "    \"A empresa reportou um preju√≠zo milion√°rio devido √† crise.\", # Esperado: üî¥ NEGATIVO\n",
        "    \"As a√ß√µes ca√≠ram drasticamente ap√≥s o esc√¢ndalo.\",         # Esperado: üî¥ NEGATIVO\n",
        "    \"O mercado fechou est√°vel, sem grandes oscila√ß√µes.\",       # Esperado: üü° NEUTRO\n",
        "    \"Company revenue increased significantly.\"                 # Esperado: üü¢ POSITIVO\n",
        "]\n",
        "\n",
        "mapa = {0: \"üî¥ NEGATIVO\", 1: \"üü° NEUTRO\", 2: \"üü¢ POSITIVO\"}\n",
        "\n",
        "for txt in testes:\n",
        "    try:\n",
        "        # Faz a predi√ß√£o\n",
        "        pred = pipeline.predict([txt])[0]\n",
        "        # Mostra o resultado\n",
        "        print(f\"'{txt[:40]}...' -> {mapa[pred]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao testar '{txt}': {e}\")\n",
        "\n",
        "print(\"\\n‚úÖ Se Preju√≠zo for Vermelho e Est√°vel for Amarelo, est√° perfeito!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e87433e8-22a4-46c6-9820-c249cbe8c9f3",
      "metadata": {
        "id": "e87433e8-22a4-46c6-9820-c249cbe8c9f3"
      },
      "source": [
        "# ============================\n",
        "# 10. CONCLUS√ÉO\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "dbbbf5af-c774-47a4-b95b-2f1a9cb8d093",
      "metadata": {
        "id": "dbbbf5af-c774-47a4-b95b-2f1a9cb8d093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "CONCLUS√ÉO FINAL DO PROJETO FINANCEIRO\n",
            "============================================================\n",
            "\n",
            "‚úÖ MELHORIAS IMPLEMENTADAS:\n",
            "   1. Suporte Bilingue (PT-BR + EN)\n",
            "   2. Corre√ß√£o da 'Armadilha do Neutro' (Balanceamento)\n",
            "   3. Calibra√ß√£o Fina ('Vacina') para termos cr√≠ticos de mercado\n",
            "   4. Detec√ß√£o precisa de Preju√≠zo (0), Estabilidade (1) e Lucro (2)\n",
            "\n",
            "üìä Performance Final: 74.1%\n",
            "üöÄ O modelo est√° pronto para ser integrado ao Backend Java!\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# 10. CONCLUS√ÉO DO PROJETO\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONCLUS√ÉO FINAL DO PROJETO FINANCEIRO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n‚úÖ MELHORIAS IMPLEMENTADAS:\")\n",
        "print(\"   1. Suporte Bilingue (PT-BR + EN)\")\n",
        "print(\"   2. Corre√ß√£o da 'Armadilha do Neutro' (Balanceamento)\")\n",
        "print(\"   3. Calibra√ß√£o Fina ('Vacina') para termos cr√≠ticos de mercado\")\n",
        "print(\"   4. Detec√ß√£o precisa de Preju√≠zo (0), Estabilidade (1) e Lucro (2)\")\n",
        "\n",
        "print(f\"\\nüìä Performance Final: {acc*100:.1f}%\")\n",
        "print(\"üöÄ O modelo est√° pronto para ser integrado ao Backend Java!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "520c18fd-0448-4f13-a784-5e72c8f9999b",
      "metadata": {
        "id": "520c18fd-0448-4f13-a784-5e72c8f9999b"
      },
      "source": [
        "README.md - AN√ÅLISE DE SENTIMENTOS EM CR√çTICAS DE FILMES"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LLOmd9C-bv0D",
      "metadata": {
        "id": "LLOmd9C-bv0D"
      },
      "source": [
        "## **Multilanguage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "ud4vi_P7byfW",
      "metadata": {
        "id": "ud4vi_P7byfW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ 11. Gerando Vers√£o de Produ√ß√£o (ONNX)...\n",
            "üß† Re-treinando com base completa (16353 frases)...\n",
            "‚úÖ ARQUIVO GERADO: c:\\Users\\Pichau\\Downloads\\PROJETOS PROGRAMA√á√ÉO\\SentimentAPI\\data-science\\sentiment_model_multilang.onnx\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# 11. EXPORTA√á√ÉO FINAL (TREINO FULL + ONNX)\n",
        "# ==============================================================================\n",
        "from skl2onnx import convert_sklearn\n",
        "from skl2onnx.common.data_types import StringTensorType\n",
        "import os\n",
        "\n",
        "print(\"üì¶ 11. Gerando Vers√£o de Produ√ß√£o (ONNX)...\")\n",
        "\n",
        "# 1. Juntar TUDO (Treino + Teste + Vacina) para o modelo final ficar o mais inteligente poss√≠vel\n",
        "df_full = pd.concat([df_final_train, pd.DataFrame({'text': X_test, 'target': y_test})])\n",
        "print(f\"üß† Re-treinando com base completa ({len(df_full)} frases)...\")\n",
        "\n",
        "pipeline.fit(df_full['text'], df_full['target'])\n",
        "\n",
        "# 2. Converter para ONNX\n",
        "initial_type = [('text_input', StringTensorType([None, 1]))]\n",
        "onnx_model = convert_sklearn(pipeline, initial_types=initial_type)\n",
        "\n",
        "# 3. Salvar\n",
        "nome_arquivo = \"sentiment_model_multilang.onnx\"\n",
        "with open(nome_arquivo, \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())\n",
        "\n",
        "print(f\"‚úÖ ARQUIVO GERADO: {os.path.abspath(nome_arquivo)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feca8b4e",
      "metadata": {},
      "source": [
        "# üé¨ An√°lise de Sentimentos em Cr√≠ticas de Filmes\n",
        "\n",
        "![Python](https://img.shields.io/badge/Python-3.8%2B-blue)\n",
        "![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.3%2B-orange)\n",
        "![NLTK](https://img.shields.io/badge/NLTK-3.8%2B-green)\n",
        "![Status](https://img.shields.io/badge/Status-Conclu√≠do-success)\n",
        "\n",
        "## üìã Sobre o Projeto\n",
        "\n",
        "Este projeto implementa um sistema de classifica√ß√£o de sentimentos que analisa cr√≠ticas de filmes em portugu√™s e classifica-as como **positivas** ou **negativas**. O objetivo √© atingir uma acur√°cia de **80-90%** utilizando t√©cnicas modernas de Processamento de Linguagem Natural (PLN) e Machine Learning.\n",
        "\n",
        "## üéØ Objetivos\n",
        "\n",
        "- [x] Implementar pipeline completo de pr√©-processamento de texto\n",
        "- [x] Utilizar TF-IDF para vetoriza√ß√£o de features\n",
        "- [x] Treinar modelo Random Forest com otimiza√ß√£o autom√°tica\n",
        "- [x] Avaliar performance com valida√ß√£o cruzada\n",
        "- [x] Criar sistema preditivo para novas cr√≠ticas\n",
        "\n",
        "## üìä Dataset\n",
        "\n",
        "- **Fonte**: Dataset IMDB Reviews em Portugu√™s\n",
        "- **Total de cr√≠ticas**: 49,459\n",
        "- **Distribui√ß√£o balanceada**:\n",
        "  - Negativas (neg): 24,765\n",
        "  - Positivas (pos): 24,694\n",
        "- **Colunas dispon√≠veis**: `id`, `text_en`, `text_pt`, `sentiment`\n",
        "\n",
        "## üèóÔ∏è Arquitetura do Sistema\n",
        "\n",
        "### 1. **Pr√©-processamento de Texto**\n",
        "```python\n",
        "Etapas do pr√©-processamento:\n",
        "1. Convers√£o para min√∫sculas\n",
        "2. Remo√ß√£o de tags HTML\n",
        "3. Filtro de caracteres especiais\n",
        "4. Tokeniza√ß√£o em portugu√™s\n",
        "5. Remo√ß√£o de stopwords\n",
        "6. Stemming (redu√ß√£o √† raiz)\n",
        "7. Reconstru√ß√£o do texto"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a23e68f7",
      "metadata": {},
      "source": [
        "### 3. **Modelo de Classifica√ß√£o**\n",
        "- **Algoritmo**: Random Forest Classifier\n",
        "- **Vantagens**:\n",
        "  - Modelo ensemble (m√∫ltiplas √°rvores)\n",
        "  - Menos propenso a overfitting\n",
        "  - Lida bem com muitas features\n",
        "- **Hiperpar√¢metros otimizados** via GridSearchCV\n",
        "\n",
        "\n",
        "### 4. **Otimiza√ß√£o Autom√°tica**\n",
        "```python\n",
        "GridSearchCV com:\n",
        "- Valida√ß√£o cruzada: 3 folds\n",
        "- M√©trica: Acur√°cia\n",
        "- Teste de m√∫ltiplos par√¢metros\n",
        "- Paraleliza√ß√£o completa\n",
        "```\n",
        "\n",
        "### 2. **Vetoriza√ß√£o TF-IDF**\n",
        "- Considera frequ√™ncia da palavra no documento\n",
        "- Penaliza palavras muito comuns\n",
        "- Captura import√¢ncia relativa das palavras\n",
        "- Configura√ß√µes otimizadas:\n",
        "  - `max_features=5000`\n",
        "  - `ngram_range=(1,2)`\n",
        "  - `min_df=5`\n",
        "  - `max_df=0.7`\n",
        "\n",
        "\n",
        "\n",
        "### 4. **Otimiza√ß√£o Autom√°tica**\n",
        "```python\n",
        "GridSearchCV com:\n",
        "- Valida√ß√£o cruzada: 3 folds\n",
        "- M√©trica: Acur√°cia\n",
        "- Teste de m√∫ltiplos par√¢metros\n",
        "- Paraleliza√ß√£o completa\n",
        "```\n",
        "\n",
        "## üìà Resultados Esperados\n",
        "\n",
        "| M√©trica | Valor Esperado |\n",
        "|---------|---------------|\n",
        "| Acur√°cia | 80-90% |\n",
        "| Precis√£o | > 85% |\n",
        "| Recall | > 85% |\n",
        "| F1-Score | > 85% |\n",
        "\n",
        "## üîß Instala√ß√£o e Execu√ß√£o\n",
        "\n",
        "### 1. Pr√©-requisitos\n",
        "```bash\n",
        "# Vers√£o do Python\n",
        "Python 3.8 ou superior\n",
        "\n",
        "# Instalar depend√™ncias\n",
        "pip install pandas numpy scikit-learn nltk\n",
        "\n",
        "# Baixar recursos do NLTK\n",
        "python -c \"import nltk; nltk.download('punkt_tab'); nltk.download('punkt'); nltk.download('stopwords')\"\n",
        "```\n",
        "\n",
        "### 2. Estrutura do Projeto\n",
        "```\n",
        "analise-sentimentos/\n",
        "‚îú‚îÄ‚îÄ AnaliseDeSentimentos.ipynb    # Notebook principal\n",
        "‚îú‚îÄ‚îÄ imdb-reviews-pt-br.csv       # Dataset\n",
        "‚îú‚îÄ‚îÄ README.md                    # Documenta√ß√£o\n",
        "‚îî‚îÄ‚îÄ requirements.txt            # Depend√™ncias\n",
        "```\n",
        "\n",
        "### 3. Execu√ß√£o\n",
        "```bash\n",
        "# Executar o notebook completo\n",
        "jupyter notebook AnaliseDeSentimentos.ipynb\n",
        "\n",
        "# Ou executar como script Python\n",
        "python AnaliseDeSentimentos.py\n",
        "```\n",
        "\n",
        "## üöÄ Como Usar o Modelo\n",
        "\n",
        "```python\n",
        "from seu_modelo import analisar_sentimento\n",
        "\n",
        "# Exemplos de uso\n",
        "criticas = [\n",
        "    \"Filme incr√≠vel! Atua√ß√µes impec√°veis.\",\n",
        "    \"Perda de tempo total, n√£o recomendo.\",\n",
        "    \"Razo√°vel, poderia ser melhor.\"\n",
        "]\n",
        "\n",
        "for critica in criticas:\n",
        "    resultado = analisar_sentimento(critica)\n",
        "    print(f\"Cr√≠tica: {critica[:50]}...\")\n",
        "    print(f\"Sentimento: {resultado['sentimento']}\")\n",
        "    print(f\"Confian√ßa: {resultado['confianca']:.2%}\")\n",
        "```\n",
        "\n",
        "## üìÅ Estrutura do C√≥digo\n",
        "\n",
        "### M√≥dulos Principais\n",
        "\n",
        "1. **`preprocessamento_avancado()`**\n",
        "   - Fun√ß√£o principal de limpeza de texto\n",
        "   - Suporte a caracteres acentuados em portugu√™s\n",
        "   - Remo√ß√£o inteligente de stopwords\n",
        "\n",
        "2. **`Pipeline` de Machine Learning**\n",
        "   - Integra√ß√£o TF-IDF + Random Forest\n",
        "   - Encapsulamento completo do fluxo\n",
        "   - Facilidade de manuten√ß√£o\n",
        "\n",
        "3. **`GridSearchCV`**\n",
        "   - Busca exaustiva de melhores par√¢metros\n",
        "   - Valida√ß√£o cruzada incorporada\n",
        "   - Paraleliza√ß√£o para performance\n",
        "\n",
        "### Fluxo de Execu√ß√£o\n",
        "```\n",
        "Carregar Dados ‚Üí Pr√©-processar ‚Üí Vetorizar ‚Üí Treinar ‚Üí Otimizar ‚Üí Avaliar ‚Üí Predizer\n",
        "```\n",
        "\n",
        "## üé® Features Implementadas\n",
        "\n",
        "### ‚úÖ Corrigidas do C√≥digo Original\n",
        "- **Pr√©-processamento**: Mant√©m palavras inteiras (n√£o letras soltas)\n",
        "- **Tokeniza√ß√£o**: Usa `punkt_tab` para portugu√™s\n",
        "- **Vetoriza√ß√£o**: TF-IDF em vez de CountVectorizer simples\n",
        "- **Modelo**: Random Forest em vez de Naive Bayes b√°sico\n",
        "\n",
        "### ‚úÖ Otimiza√ß√µes Adicionais\n",
        "- Pipeline organizado com Scikit-learn\n",
        "- Otimiza√ß√£o autom√°tica de hiperpar√¢metros\n",
        "- Valida√ß√£o cruzada para avalia√ß√£o robusta\n",
        "- An√°lise detalhada de erros\n",
        "\n",
        "## üìä An√°lise de Desempenho\n",
        "\n",
        "### M√©tricas de Avalia√ß√£o\n",
        "- **Acur√°cia**: Porcentagem de classifica√ß√µes corretas\n",
        "- **Precis√£o**: Entre as classificadas como positivas, quantas realmente s√£o\n",
        "- **Recall**: Entre todas as positivas reais, quantas foram identificadas\n",
        "- **F1-Score**: M√©dia harm√¥nica entre precis√£o e recall\n",
        "\n",
        "### Matriz de Confus√£o\n",
        "```\n",
        "              Predito Negativo  Predito Positivo\n",
        "Real Negativo      TN                FP\n",
        "Real Positivo      FN                TP\n",
        "```\n",
        "\n",
        "## üîÑ Pr√≥ximas Melhorias\n",
        "\n",
        "### 1. Engenharia de Features Avan√ßada\n",
        "- [ ] Contagem de palavras positivas/negativas\n",
        "- [ ] Extra√ß√£o de emoticons e exclama√ß√µes\n",
        "- [ ] An√°lise de senten√ßas por par√°grafo\n",
        "\n",
        "### 2. Modelos Avan√ßados\n",
        "- [ ] XGBoost ou LightGBM\n",
        "- [ ] SVM com kernel n√£o-linear\n",
        "- [ ] Redes Neurais (MLP)\n",
        "\n",
        "### 3. Deep Learning\n",
        "- [ ] LSTM/GRU para contexto sequencial\n",
        "- [ ] BERTimbau (BERT em portugu√™s)\n",
        "- [ ] Fine-tuning de transformers\n",
        "\n",
        "### 4. Sistema em Produ√ß√£o\n",
        "- [ ] API REST com FastAPI\n",
        "- [ ] Sistema de cache de predi√ß√µes\n",
        "- [ ] Monitoramento de performance\n",
        "- [ ] Logs detalhados\n",
        "\n",
        "## üìù Conclus√£o\n",
        "\n",
        "Este projeto demonstra uma implementa√ß√£o completa de an√°lise de sentimentos, abordando desde o pr√©-processamento b√°sico at√© otimiza√ß√µes avan√ßadas. A arquitetura modular permite f√°cil extens√£o e adapta√ß√£o para diferentes dom√≠nios.\n",
        "\n",
        "### Principais Aprendizados\n",
        "1. **Pr√©-processamento √© crucial**: Representa√ß√£o correta dos dados afeta diretamente os resultados\n",
        "2. **TF-IDF > CountVectorizer**: Considera import√¢ncia relativa das palavras\n",
        "3. **Random Forest robusto**: Excelente para problemas de classifica√ß√£o de texto\n",
        "4. **Otimiza√ß√£o sistem√°tica**: GridSearchCV encontra automaticamente os melhores par√¢metros\n",
        "\n",
        "## üë• Contribui√ß√£o\n",
        "\n",
        "Contribui√ß√µes s√£o bem-vindas! Siga estes passos:\n",
        "\n",
        "1. Fork do reposit√≥rio\n",
        "2. Crie uma branch (`git checkout -b feature/nova-feature`)\n",
        "3. Commit suas mudan√ßas (`git commit -m 'Add nova feature'`)\n",
        "4. Push para a branch (`git push origin feature/nova-feature`)\n",
        "5. Abra um Pull Request\n",
        "\n",
        "## üìÑ Licen√ßa\n",
        "\n",
        "Este projeto est√° sob a licen√ßa MIT. Veja o arquivo [LICENSE](LICENSE) para detalhes.\n",
        "\n",
        "## üôè Agradecimentos\n",
        "\n",
        "- Dataset: [IMDB Reviews em Portugu√™s](https://www.kaggle.com/datasets)\n",
        "- Bibliotecas: Scikit-learn, NLTK, Pandas, NumPy\n",
        "- Comunidade de Data Science\n",
        "\n",
        "## üìû Contato\n",
        "\n",
        "Para d√∫vidas ou sugest√µes, entre em contato:\n",
        "\n",
        "**Desenvolvedor**: [Seu Nome]  \n",
        "**Email**: seu.email@exemplo.com  \n",
        "**LinkedIn**: [linkedin.com/in/seu-perfil](https://linkedin.com)\n",
        "\n",
        "---\n",
        "*\"Transformando texto em insights atrav√©s de dados\"* üöÄ\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **PRINCIPAIS CORRE√á√ïES APLICADAS:**\n",
        "\n",
        "1. **Corrigido erro do NLTK**: Adicionado download do `punkt_tab`\n",
        "2. **Sequ√™ncia l√≥gica**: Garantida execu√ß√£o na ordem correta\n",
        "3. **Simplifica√ß√£o**: Reduzida complexidade do GridSearchCV para execu√ß√£o mais r√°pida\n",
        "4. **Manuten√ß√£o de contexto**: Todas as vari√°veis s√£o definidas antes do uso\n",
        "\n",
        "## **PR√ìXIMOS PASSOS SUGERIDOS:**\n",
        "\n",
        "1. **Salvar o modelo treinado**:\n",
        "```python\n",
        "import joblib\n",
        "joblib.dump(grid_search, 'modelo_sentimentos.pkl')\n",
        "```\n",
        "\n",
        "2. **Criar API**:\n",
        "```python\n",
        "from fastapi import FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/analisar\")\n",
        "def analisar(critica: str):\n",
        "    texto_limpo = preprocessamento_avancado(critica)\n",
        "    predicao = grid_search.predict([texto_limpo])[0]\n",
        "    return {\"sentimento\": \"positivo\" if predicao == 1 else \"negativo\"}\n",
        "```\n",
        "\n",
        "3. **Monitoramento**:\n",
        "   - Adicionar logging\n",
        "   - Implementar tracking de performance\n",
        "   - Criar dashboard de m√©tricas\n",
        "\n",
        "O projeto est√° agora funcional e pronto para execu√ß√£o!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
