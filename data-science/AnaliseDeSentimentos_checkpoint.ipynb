{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "70f12a0f-70e2-4605-8b4f-4b23b5b387c3",
      "metadata": {
        "id": "70f12a0f-70e2-4605-8b4f-4b23b5b387c3"
      },
      "source": [
        "# ============================\n",
        "\n",
        "# An√°lise de sentimentos\n",
        "Hoje, as empresas buscam compreender os pontos fracos de seus lan√ßamentos e a percep√ß√£o do p√∫blico sobre seus servi√ßos, produtos e marca. Para isso, podem contar com a an√°lise de sentimento, uma t√©cnica que mede com precis√£o as opini√µes expressas em textos, como coment√°rios e avalia√ß√µes.\n",
        "\n",
        "Essa an√°lise pode ser feita com o aux√≠lio de ferramentas prontas (como RandomForestClassifie ,TfidfVectorizer e NLTK) ou com modelos treinados para um setor espec√≠fico. Al√©m disso, atualmente √© poss√≠vel ir al√©m: √© vi√°vel organizar automaticamente os coment√°rios por temas ‚Äî identificando, por exemplo, men√ß√µes a \"atendimento\", \"pre√ßo\" ou \"qualidade\" ‚Äî mesmo sem ter classifica√ß√µes pr√©vias, utilizando m√©todos de aprendizado n√£o supervisionado.\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c5004662",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.5)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.8.0)\n",
            "Requirement already satisfied: skl2onnx in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.19.1)\n",
            "Requirement already satisfied: onnxmltools in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pichau\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: onnx>=1.2.1 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from skl2onnx) (1.20.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnx>=1.2.1->skl2onnx) (6.33.2)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnx>=1.2.1->skl2onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnx>=1.2.1->skl2onnx) (0.5.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\pichau\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Instala o pacote b√°sico de Data Science + as ferramentas para exportar pro Java\n",
        "%pip install pandas numpy scikit-learn skl2onnx onnxmltools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d983439b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
            "Requirement already satisfied: click in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\pichau\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "497269dd-8602-44b0-b442-1d81c7abb792",
      "metadata": {
        "id": "497269dd-8602-44b0-b442-1d81c7abb792"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 1. IMPORTANDO BIBLIOTECAS\n",
        "# ============================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "51d5da3f-1e23-4607-ae90-8a898ccb8549",
      "metadata": {
        "id": "51d5da3f-1e23-4607-ae90-8a898ccb8549"
      },
      "outputs": [],
      "source": [
        "# Ferramentas espec√≠ficas do NLTK\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Ferramentas do Scikit-learn para machine learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33251a44-93a3-44f9-89c7-5530a36d66db",
      "metadata": {
        "id": "33251a44-93a3-44f9-89c7-5530a36d66db"
      },
      "source": [
        "# ============================\n",
        "# 2. BAIXAR RECURSOS DO NLTK\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d2b74135-47dd-4fa3-b5cc-4ba7c7c517a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2b74135-47dd-4fa3-b5cc-4ba7c7c517a6",
        "outputId": "65e39121-9be3-4401-9955-5a5e7f75b987"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baixando recursos do NLTK...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Pichau\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Pichau\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Pichau\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Baixando recursos do NLTK...\")\n",
        "nltk.download('punkt_tab')  # Adicionado: necess√°rio para tokeniza√ß√£o em portugu√™s\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c64eeb67-15cb-4d9c-9cc0-194dd59372fe",
      "metadata": {
        "id": "c64eeb67-15cb-4d9c-9cc0-194dd59372fe"
      },
      "source": [
        "# ============================\n",
        "# 3. CARREGAR OS DADOS\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bc6ffa60-34f2-4998-b7c2-f3d28d91dc03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc6ffa60-34f2-4998-b7c2-f3d28d91dc03",
        "outputId": "ff24dc1f-bb56-431a-fdb4-d4d167d821a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Carregando dados do GitHub LFS...\n",
            "üéâ SUCESSO! 4845 linhas baixadas da nuvem.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "print(\"üöÄ Carregando dados do GitHub LFS...\")\n",
        "\n",
        "url_github = \"https://media.githubusercontent.com/media/cauasantoslt/SentimentAPI/refs/heads/main/data-science/financial_phrase_bank_pt_br.csv\"\n",
        "\n",
        "try:\n",
        "    # O Pandas vai baixar os 120MB direto desse link\n",
        "    data = pd.read_csv(url_github, on_bad_lines='skip')\n",
        "    print(f\"üéâ SUCESSO! {len(data)} linhas baixadas da nuvem.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Erro ao baixar: {e}\")\n",
        "    print(\"Tentando ajustar o link...\")\n",
        "    \n",
        "    # Tenta uma varia√ß√£o comum se o primeiro falhar (sem o 'refs/heads')\n",
        "    try:\n",
        "        url_fallback = \"https://media.githubusercontent.com/media/cauasantoslt/SentimentAPI/refs/heads/main/data-science/financial_phrase_bank_pt_br.csv\"\n",
        "        data = pd.read_csv(url_fallback, on_bad_lines='skip')\n",
        "        print(f\"üéâ SUCESSO NA SEGUNDA TENTATIVA! {len(data)} linhas.\")\n",
        "    except:\n",
        "        print(\"‚ùå N√£o foi poss√≠vel baixar. Verifique se o reposit√≥rio √© P√∫blico.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb44b898-4d40-439d-a17d-3661dcfe81e4",
      "metadata": {
        "id": "cb44b898-4d40-439d-a17d-3661dcfe81e4"
      },
      "source": [
        "# ============================\n",
        "# 4. PR√â-PROCESSAMENTO MELHORADO\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3591745a-6caf-46b4-a3e6-40a7c104ebea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3591745a-6caf-46b4-a3e6-40a7c104ebea",
        "outputId": "58701c99-f63c-4176-aa48-85b40cfc31a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processando textos...\n",
            "\n",
            "Exemplo de pr√©-processamento:\n",
            "Original (primeiras 200 caracteres):\n",
            "A Technopolis planeja desenvolver em etapas uma √°rea n√£o inferior a 100 mil metros quadrados para hospedar empresas que atuam em tecnologias de inform√°tica e telecomunica√ß√µes, afirma o comunicado.\n",
            "\n",
            "Limpo:\n",
            "technopol planej desenvolv etap √°re inferior mil metr quadr hosped empres atu tecnolog inform√°t telecomunic afirm comunic\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def preprocessamento_avancado(texto):\n",
        "    \"\"\"\n",
        "    Limpa e prepara o texto para an√°lise.\n",
        "    Vers√£o corrigida: mant√©m palavras inteiras, n√£o letras soltas.\n",
        "    \"\"\"\n",
        "    # Verifica se √© texto v√°lido\n",
        "    if not isinstance(texto, str):\n",
        "        return \"\"\n",
        "\n",
        "    # 1. Converter para min√∫sculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # 2. Remover tags HTML (se houver)\n",
        "    texto = re.sub(r'<.*?>', ' ', texto)\n",
        "\n",
        "    # 3. Manter apenas letras (com acentos) e espa√ßos\n",
        "    texto = re.sub(r'[^a-z√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√ß√†√®√¨√≤√π\\\\s]', ' ', texto)\n",
        "\n",
        "    # 4. Remover espa√ßos m√∫ltiplos\n",
        "    texto = re.sub(r'\\\\s+', ' ', texto)\n",
        "\n",
        "    # 5. Tokeniza√ß√£o: dividir em palavras individuais\n",
        "    palavras = word_tokenize(texto, language='portuguese')\n",
        "\n",
        "    # 6. Remover stopwords (palavras irrelevantes)\n",
        "    stop_words = set(stopwords.words('portuguese'))\n",
        "    palavras_filtradas = [p for p in palavras if p not in stop_words and len(p) > 2]\n",
        "\n",
        "    # 7. Stemming: reduzir palavras √† raiz\n",
        "    stemmer = SnowballStemmer('portuguese')\n",
        "    palavras_stem = [stemmer.stem(p) for p in palavras_filtradas]\n",
        "\n",
        "    # 8. Juntar palavras novamente em um texto\n",
        "    return ' '.join(palavras_stem)\n",
        "\n",
        "print(\"Processando textos...\")\n",
        "# Aplicar a fun√ß√£o de pr√©-processamento a cada cr√≠tica\n",
        "data['texto_limpo'] = data['text_pt'].apply(preprocessamento_avancado)\n",
        "\n",
        "# Mostrar exemplo do pr√©-processamento\n",
        "print(\"\\nExemplo de pr√©-processamento:\")\n",
        "print(\"Original (primeiras 200 caracteres):\")\n",
        "print(data['text_pt'].iloc[0][:200])\n",
        "print(\"\\nLimpo:\")\n",
        "print(data['texto_limpo'].iloc[0][:200])\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1052693b-270e-45e1-8fe3-d71e9df4ce45",
      "metadata": {
        "id": "1052693b-270e-45e1-8fe3-d71e9df4ce45"
      },
      "source": [
        "# ============================\n",
        "# 5. PREPARAR DADOS PARA MODELO\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1e7cd69a-ea3f-4549-9273-1cd865fa57cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e7cd69a-ea3f-4549-9273-1cd865fa57cb",
        "outputId": "2c1814f2-73d7-4545-d917-1f0a3d10ce62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Preparando X e y para a base Financeira...\n",
            "‚úÖ Coluna de sentimento detectada: 'y'\n",
            "‚úÖ Usando coluna 'texto_limpo' para o treinamento.\n",
            "\n",
            "Dados Prontos:\n",
            "X (Textos): (4845,)\n",
            "y (Labels): (4845,)\n",
            "Exemplos de y: [1 0 2 2 2 2 2 2 2 2] (0=Neg, 1=Neu, 2=Pos)\n"
          ]
        }
      ],
      "source": [
        "print(\"üìä Preparando X e y para a base Financeira...\")\n",
        "\n",
        "# 1. Identificar a coluna de sentimento (pode ser 'sentiment' ou 'y')\n",
        "coluna_alvo = 'sentiment'\n",
        "if 'sentiment' not in data.columns:\n",
        "    if 'y' in data.columns:\n",
        "        coluna_alvo = 'y'\n",
        "    else:\n",
        "        # Pega a √∫ltima coluna como palpite se n√£o achar pelo nome\n",
        "        coluna_alvo = data.columns[-1]\n",
        "\n",
        "print(f\"‚úÖ Coluna de sentimento detectada: '{coluna_alvo}'\")\n",
        "\n",
        "# 2. Mapeamento Atualizado (Agora com NEUTRO!)\n",
        "# A base financeira usa 'negative', 'neutral', 'positive'\n",
        "mapa_sentimentos = {\n",
        "    'negative': 0,\n",
        "    'neutral': 1,  # <--- O Neutro entra aqui\n",
        "    'positive': 2,\n",
        "    # Casos extras de seguran√ßa (se vier abreviado)\n",
        "    'neg': 0,\n",
        "    'pos': 2,\n",
        "    'neu': 1\n",
        "}\n",
        "\n",
        "# 3. Aplicar a convers√£o\n",
        "# Convertemos para string e min√∫sculo para garantir que o mapa funcione\n",
        "try:\n",
        "    y = data[coluna_alvo].astype(str).str.lower().map(mapa_sentimentos).values\n",
        "    \n",
        "    # Verifica se gerou algum NaN (sentimento que n√£o estava no mapa)\n",
        "    if np.isnan(y).any():\n",
        "        print(\"‚ö†Ô∏è Aten√ß√£o: Alguns sentimentos n√£o foram mapeados e viraram NaN. Removendo...\")\n",
        "        # L√≥gica para remover NaNs se necess√°rio, mas geralmente o dropna anterior resolveu\n",
        "        # Aqui vamos apenas preencher com neutro (1) para n√£o quebrar o c√≥digo\n",
        "        y = np.nan_to_num(y, nan=1).astype(int)\n",
        "    else:\n",
        "        y = y.astype(int)\n",
        "\n",
        "    # 4. Definir X (Entrada)\n",
        "    # Usa o texto limpo se existir, sen√£o usa o texto cru\n",
        "    if 'texto_limpo' in data.columns:\n",
        "        X = data['texto_limpo'].values\n",
        "        print(\"‚úÖ Usando coluna 'texto_limpo' para o treinamento.\")\n",
        "    else:\n",
        "        # Tenta achar a coluna de texto original\n",
        "        col_texto = 'text_pt' if 'text_pt' in data.columns else 'text_en'\n",
        "        X = data[col_texto].values\n",
        "        print(f\"‚ö†Ô∏è 'texto_limpo' n√£o encontrado. Usando '{col_texto}' sem limpeza.\")\n",
        "\n",
        "    print(f\"\\nDados Prontos:\")\n",
        "    print(f\"X (Textos): {X.shape}\")\n",
        "    print(f\"y (Labels): {y.shape}\")\n",
        "    print(f\"Exemplos de y: {y[:10]} (0=Neg, 1=Neu, 2=Pos)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro ao preparar dados: {e}\")\n",
        "    print(f\"Colunas dispon√≠veis no dataframe: {data.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81ea9918-61a6-4ec1-8cca-afc3ccb86cc1",
      "metadata": {
        "id": "81ea9918-61a6-4ec1-8cca-afc3ccb86cc1"
      },
      "source": [
        "# ============================\n",
        "# 6. DIVIDIR DADOS EM TREINO E TESTE\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "KkbI5mV6erSX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkbI5mV6erSX",
        "outputId": "459a31f0-b886-4f2c-a59d-647942cefa98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßπ Iniciando limpeza de dados nulos...\n",
            "‚úÖ Dados higienizados com sucesso!\n",
            "Shape de X: (49459,)\n",
            "Shape de y: (49459,)\n"
          ]
        }
      ],
      "source": [
        "# --- LIMPEZA E PREPARA√á√ÉO DOS DADOS ---\n",
        "print(\"üßπ Iniciando limpeza de dados nulos...\")\n",
        "\n",
        "# 1. Remove linhas onde o texto ou o sentimento vieram vazios do CSV\n",
        "data = data.dropna(subset=['text_pt', 'sentiment'])\n",
        "\n",
        "# 2. Converte sentimentos para n√∫meros (0 = negativo, 1 = positivo)\n",
        "# Se houver algum sentimento escrito errado (ex: \"neutro\"), o map vai gerar NaN\n",
        "data['target_final'] = data['sentiment'].map({'neg': 0, 'pos': 1})\n",
        "\n",
        "# 3. O PULO DO GATO: Remove linhas onde a convers√£o falhou (NaN)\n",
        "data = data.dropna(subset=['target_final'])\n",
        "\n",
        "# 4. Define X e y apenas com dados 100% limpos\n",
        "y = data['target_final'].values\n",
        "# Se voc√™ estiver usando o multilanguage, lembre de ajustar X depois.\n",
        "# Se for o notebook original do Jefferson:\n",
        "if 'texto_limpo' in data.columns:\n",
        "    X = data['texto_limpo'].values\n",
        "else:\n",
        "    X = data['text_pt'].values\n",
        "\n",
        "print(f\"‚úÖ Dados higienizados com sucesso!\")\n",
        "print(f\"Shape de X: {X.shape}\")\n",
        "print(f\"Shape de y: {y.shape}\")\n",
        "\n",
        "# Agora sim, pode rodar o train_test_split sem medo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5648df49-8c97-46e8-aa5a-87afd0faeb81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5648df49-8c97-46e8-aa5a-87afd0faeb81",
        "outputId": "9201d9f4-df7b-44f8-ab64-743237171b75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Divis√£o dos dados:\n",
            "Treino: 3876 amostras\n",
            "Teste: 969 amostras\n",
            "Propor√ß√£o positiva no treino: 115.69%\n",
            "Propor√ß√£o positiva no teste: 115.58%\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nDivis√£o dos dados:\")\n",
        "print(f\"Treino: {len(X_train)} amostras\")\n",
        "print(f\"Teste: {len(X_test)} amostras\")\n",
        "print(f\"Propor√ß√£o positiva no treino: {y_train.mean():.2%}\")\n",
        "print(f\"Propor√ß√£o positiva no teste: {y_test.mean():.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2491cc8f-2c68-467e-94db-de1838bee5a4",
      "metadata": {
        "id": "2491cc8f-2c68-467e-94db-de1838bee5a4"
      },
      "source": [
        "# ============================\n",
        "# 7. CRIAR E OTIMIZAR O PIPELINE\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a47d5e3c-9fcb-482f-afa3-5d6e78b64c0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a47d5e3c-9fcb-482f-afa3-5d6e78b64c0a",
        "outputId": "ccb12fd5-de5d-4fe3-9963-08eda55f4192"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö° Configurando Pipeline para Base Financeira (3 Classes)...\n",
            "‚úÇÔ∏è Separando dados de treino e teste...\n",
            "üèãÔ∏è Treinando agora (vai ser r√°pido)...\n",
            "‚úÖ Treinamento conclu√≠do em 0.23 segundos!\n",
            "\n",
            "üìä RESULTADOS DO TREINO:\n",
            "üéØ Acur√°cia: 76.57%\n",
            "\n",
            "Relat√≥rio Detalhado:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negativo       0.81      0.49      0.61       121\n",
            "      Neutro       0.76      0.95      0.84       576\n",
            "    Positivo       0.78      0.50      0.61       272\n",
            "\n",
            "    accuracy                           0.77       969\n",
            "   macro avg       0.78      0.65      0.69       969\n",
            "weighted avg       0.77      0.77      0.75       969\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import time\n",
        "\n",
        "print(\"‚ö° Configurando Pipeline para Base Financeira (3 Classes)...\")\n",
        "\n",
        "# 1. VERIFICA√á√ÉO DE SEGURAN√áA\n",
        "if 'X' not in locals() or 'y' not in locals():\n",
        "    print(\"‚ùå ERRO: As vari√°veis X e y n√£o foram encontradas.\")\n",
        "    print(\"üëâ Por favor, rode a Etapa 5 novamente.\")\n",
        "else:\n",
        "    # 2. SEPARA√á√ÉO TREINO E TESTE\n",
        "    print(\"‚úÇÔ∏è Separando dados de treino e teste...\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # 3. CRIA√á√ÉO DO PIPELINE\n",
        "    pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(max_features=5000)), \n",
        "        ('clf', LogisticRegression(\n",
        "            solver='lbfgs', \n",
        "            # multi_class='auto', <--- REMOVIDO (A IA agora faz isso sozinha)\n",
        "            max_iter=500,       \n",
        "            n_jobs=-1           \n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    # 4. TREINAMENTO\n",
        "    print(\"üèãÔ∏è Treinando agora (vai ser r√°pido)...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    pipeline.fit(X_train, y_train)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    print(f\"‚úÖ Treinamento conclu√≠do em {end_time - start_time:.2f} segundos!\")\n",
        "\n",
        "    # 5. AVALIA√á√ÉO\n",
        "    print(\"\\nüìä RESULTADOS DO TREINO:\")\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    print(f\"üéØ Acur√°cia: {acc*100:.2f}%\")\n",
        "    print(\"\\nRelat√≥rio Detalhado:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Negativo', 'Neutro', 'Positivo']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cf3fa7b-b3cd-4379-a4d1-f929c83c1329",
      "metadata": {
        "id": "6cf3fa7b-b3cd-4379-a4d1-f929c83c1329"
      },
      "source": [
        "# ============================\n",
        "# 8. AVALIAR RESULTADOS\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c72be8c9-4837-4cf7-be5c-e6cac1bbe2dd",
      "metadata": {
        "id": "c72be8c9-4837-4cf7-be5c-e6cac1bbe2dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä AVALIA√á√ÉO DO MODELO FINANCEIRO\n",
            "üéØ Acur√°cia Global: 76.57%\n",
            "\n",
            "üìã Relat√≥rio por Classe:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negativo       0.81      0.49      0.61       121\n",
            "      Neutro       0.76      0.95      0.84       576\n",
            "    Positivo       0.78      0.50      0.61       272\n",
            "\n",
            "    accuracy                           0.77       969\n",
            "   macro avg       0.78      0.65      0.69       969\n",
            "weighted avg       0.77      0.77      0.75       969\n",
            "\n",
            "\n",
            "üß© Matriz de Confus√£o (Linha=Real, Coluna=Previsto):\n",
            "[[ 59  50  12]\n",
            " [  4 546  26]\n",
            " [ 10 125 137]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "print(\"üìä AVALIA√á√ÉO DO MODELO FINANCEIRO\")\n",
        "\n",
        "try:\n",
        "    # 1. Faz as previs√µes nos dados de teste\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # 2. Calcula a Acur√°cia\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"üéØ Acur√°cia Global: {acc*100:.2f}%\")\n",
        "\n",
        "    # 3. Relat√≥rio Detalhado (AQUI ESTAVA O ERRO)\n",
        "    # Agora passamos 3 nomes para as classes 0, 1 e 2\n",
        "    print(\"\\nüìã Relat√≥rio por Classe:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Negativo', 'Neutro', 'Positivo']))\n",
        "\n",
        "    # 4. Matriz de Confus√£o (Opcional, mas legal de ver)\n",
        "    print(\"\\nüß© Matriz de Confus√£o (Linha=Real, Coluna=Previsto):\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "except NameError:\n",
        "    print(\"‚ùå ERRO: As vari√°veis de teste (X_test, y_test) ou o modelo (pipeline) n√£o existem.\")\n",
        "    print(\"üëâ Rode as etapas 5 e 7 novamente.\")\n",
        "except ValueError as e:\n",
        "    print(f\"‚ùå ERRO DE VALOR: {e}\")\n",
        "    print(\"Dica: Verifique se seus dados de teste realmente t√™m as 3 classes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9895e332-2510-4645-a822-e356f83534c2",
      "metadata": {
        "id": "9895e332-2510-4645-a822-e356f83534c2"
      },
      "source": [
        "# ============================\n",
        "# 9. EXEMPLO DE USO DO MODELO\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ae2a2a00-d72a-49b9-8155-77b1ef654371",
      "metadata": {
        "id": "ae2a2a00-d72a-49b9-8155-77b1ef654371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "COMO USAR O MODELO PARA NOVAS CR√çTICAS (Vers√£o Final)\n",
            "============================================================\n",
            "\n",
            "Testando o modelo com exemplos novos:\n",
            "\n",
            "Exemplo 1:\n",
            "Texto: Este filme √© incr√≠vel! A atua√ß√£o foi perfeita e a hist√≥ria emocionante.\n",
            "Sentimento previsto: POSITIVO\n",
            "\n",
            "Exemplo 2:\n",
            "Texto: Que decep√ß√£o! Perdi duas horas da minha vida com este filme ruim.\n",
            "Sentimento previsto: POSITIVO\n",
            "\n",
            "Exemplo 3:\n",
            "Texto: N√£o gostei muito, mas tem alguns momentos bons.\n",
            "Sentimento previsto: POSITIVO\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMO USAR O MODELO PARA NOVAS CR√çTICAS (Vers√£o Final)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Exemplo de predi√ß√£o\n",
        "exemplos = [\n",
        "    \"Este filme √© incr√≠vel! A atua√ß√£o foi perfeita e a hist√≥ria emocionante.\",\n",
        "    \"Que decep√ß√£o! Perdi duas horas da minha vida com este filme ruim.\",\n",
        "    \"N√£o gostei muito, mas tem alguns momentos bons.\"\n",
        "]\n",
        "\n",
        "print(\"\\nTestando o modelo com exemplos novos:\")\n",
        "for i, texto in enumerate(exemplos):\n",
        "    try:\n",
        "        # 1. Limpa o texto usando a fun√ß√£o QUE J√Å EXISTE no seu notebook\n",
        "        texto_processado = preprocessamento_avancado(texto)\n",
        "        \n",
        "        # 2. Faz a predi√ß√£o\n",
        "        predicao = pipeline.predict([texto_processado])[0]\n",
        "        \n",
        "        sentimento = \"POSITIVO\" if predicao == 1 else \"NEGATIVO\"\n",
        "        \n",
        "        print(f\"\\nExemplo {i+1}:\")\n",
        "        print(f\"Texto: {texto}\")\n",
        "        print(f\"Sentimento previsto: {sentimento}\")\n",
        "        \n",
        "    except NameError as e:\n",
        "        print(f\"‚ùå ERRO: {e}\")\n",
        "        print(\"Dica: Rode a C√©lula 8 (onde tem 'def preprocessamento_avancado') antes desta.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro gen√©rico: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e87433e8-22a4-46c6-9820-c249cbe8c9f3",
      "metadata": {
        "id": "e87433e8-22a4-46c6-9820-c249cbe8c9f3"
      },
      "source": [
        "# ============================\n",
        "# 10. CONCLUS√ÉO\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "dbbbf5af-c774-47a4-b95b-2f1a9cb8d093",
      "metadata": {
        "id": "dbbbf5af-c774-47a4-b95b-2f1a9cb8d093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "RESUMO DO PROJETO\n",
            "============================================================\n",
            "\n",
            "‚úÖ O QUE FOI FEITO:\n",
            "   1. Corrigido pr√©-processamento (palavras inteiras)\n",
            "   2. Implementado TF-IDF (melhor que CountVectorizer)\n",
            "   3. Usado Random Forest (mais robusto que Naive Bayes)\n",
            "   4. Otimizado hiperpar√¢metros com GridSearchCV\n",
            "   5. Avalia√ß√£o rigorosa com valida√ß√£o cruzada\n",
            "\n",
            "üìä RESULTADO: 76.6% de acur√°cia\n",
            "\n",
            "üìà PARA MELHORAR: 76.6%\n",
            "   Implemente as sugest√µes da se√ß√£o 'Pr√≥ximos Passos'\n",
            "\n",
            "============================================================\n",
            "FIM DA AN√ÅLISE\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESUMO DO PROJETO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n‚úÖ O QUE FOI FEITO:\")\n",
        "print(\"   1. Corrigido pr√©-processamento (palavras inteiras)\")\n",
        "print(\"   2. Implementado TF-IDF (melhor que CountVectorizer)\")\n",
        "print(\"   3. Usado Random Forest (mais robusto que Naive Bayes)\")\n",
        "print(\"   4. Otimizado hiperpar√¢metros com GridSearchCV\")\n",
        "print(\"   5. Avalia√ß√£o rigorosa com valida√ß√£o cruzada\")\n",
        "\n",
        "print(f\"\\nüìä RESULTADO: {acuracia_teste*100:.1f}% de acur√°cia\")\n",
        "\n",
        "if acuracia_teste > 0.8:\n",
        "    print(\"\\nüéâ PARAB√âNS! Meta de 80% atingida!\")\n",
        "    print(\"   Para 90%, considere as sugest√µes de melhoria.\")\n",
        "else:\n",
        "    print(f\"\\nüìà PARA MELHORAR: {acuracia_teste*100:.1f}%\")\n",
        "    print(\"   Implemente as sugest√µes da se√ß√£o 'Pr√≥ximos Passos'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FIM DA AN√ÅLISE\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "520c18fd-0448-4f13-a784-5e72c8f9999b",
      "metadata": {
        "id": "520c18fd-0448-4f13-a784-5e72c8f9999b"
      },
      "source": [
        "README.md - AN√ÅLISE DE SENTIMENTOS EM CR√çTICAS DE FILMES"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LLOmd9C-bv0D",
      "metadata": {
        "id": "LLOmd9C-bv0D"
      },
      "source": [
        "## **Multilanguage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "ud4vi_P7byfW",
      "metadata": {
        "id": "ud4vi_P7byfW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚öôÔ∏è INICIANDO PIPELINE DE PRODU√á√ÉO...\n",
            "üìä Dataset processado: 9690 frases.\n",
            "ü§ñ Modelo treinado com sucesso!\n",
            "‚úÖ ARQUIVO PRONTO: c:\\Users\\Pichau\\Downloads\\PROJETOS PROGRAMA√á√ÉO\\SentimentAPI\\data-science\\sentiment_model_multilang.onnx\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# üöÄ SCRIPT FINAL: TRAINING + EXPORT (CLEAN VERSION)\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from skl2onnx import convert_sklearn\n",
        "from skl2onnx.common.data_types import StringTensorType\n",
        "\n",
        "print(\"‚öôÔ∏è INICIANDO PIPELINE DE PRODU√á√ÉO...\")\n",
        "\n",
        "# 1. CARREGAR DADOS\n",
        "url = \"https://media.githubusercontent.com/media/cauasantoslt/SentimentAPI/refs/heads/main/data-science/financial_phrase_bank_pt_br.csv\"\n",
        "try:\n",
        "    df = pd.read_csv(url, on_bad_lines='skip', sep=None, engine='python')\n",
        "except:\n",
        "    df = pd.read_csv(url, on_bad_lines='skip')\n",
        "\n",
        "# 2. PADRONIZAR COLUNAS (PT + EN)\n",
        "df = df.rename(columns={\n",
        "    'sentence': 'text_en', \n",
        "    'english': 'text_en',\n",
        "    'text': 'text_en',\n",
        "    'y': 'sentiment'\n",
        "})\n",
        "\n",
        "# 3. MAPEAR SENTIMENTOS (0, 1, 2)\n",
        "col_sentimento = 'sentiment' if 'sentiment' in df.columns else df.columns[-1]\n",
        "df['target'] = df[col_sentimento].astype(str).str.lower().map({\n",
        "    'negative': 0, 'neg': 0,\n",
        "    'neutral': 1,  'neu': 1,\n",
        "    'positive': 2, 'pos': 2\n",
        "})\n",
        "df = df.dropna(subset=['target'])\n",
        "\n",
        "# 4. UNIFICAR IDIOMAS\n",
        "dfs = []\n",
        "if 'text_pt' in df.columns: \n",
        "    dfs.append(df[['text_pt', 'target']].rename(columns={'text_pt': 'text'}))\n",
        "if 'text_en' in df.columns: \n",
        "    dfs.append(df[['text_en', 'target']].rename(columns={'text_en': 'text'}))\n",
        "\n",
        "df_final = pd.concat(dfs, ignore_index=True)\n",
        "print(f\"üìä Dataset processado: {len(df_final)} frases.\")\n",
        "\n",
        "# 5. TREINAR MODELO (BALANCEADO)\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
        "    ('clf', LogisticRegression(solver='lbfgs', max_iter=500, class_weight='balanced'))\n",
        "])\n",
        "\n",
        "pipeline.fit(df_final['text'], df_final['target'])\n",
        "print(\"ü§ñ Modelo treinado com sucesso!\")\n",
        "\n",
        "# 6. EXPORTAR ONNX\n",
        "initial_type = [('text_input', StringTensorType([None, 1]))]\n",
        "onnx_model = convert_sklearn(pipeline, initial_types=initial_type)\n",
        "\n",
        "nome_arquivo = \"sentiment_model_multilang.onnx\"\n",
        "with open(nome_arquivo, \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())\n",
        "\n",
        "print(f\"‚úÖ ARQUIVO PRONTO: {os.path.abspath(nome_arquivo)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feca8b4e",
      "metadata": {},
      "source": [
        "# üé¨ An√°lise de Sentimentos em Cr√≠ticas de Filmes\n",
        "\n",
        "![Python](https://img.shields.io/badge/Python-3.8%2B-blue)\n",
        "![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.3%2B-orange)\n",
        "![NLTK](https://img.shields.io/badge/NLTK-3.8%2B-green)\n",
        "![Status](https://img.shields.io/badge/Status-Conclu√≠do-success)\n",
        "\n",
        "## üìã Sobre o Projeto\n",
        "\n",
        "Este projeto implementa um sistema de classifica√ß√£o de sentimentos que analisa cr√≠ticas de filmes em portugu√™s e classifica-as como **positivas** ou **negativas**. O objetivo √© atingir uma acur√°cia de **80-90%** utilizando t√©cnicas modernas de Processamento de Linguagem Natural (PLN) e Machine Learning.\n",
        "\n",
        "## üéØ Objetivos\n",
        "\n",
        "- [x] Implementar pipeline completo de pr√©-processamento de texto\n",
        "- [x] Utilizar TF-IDF para vetoriza√ß√£o de features\n",
        "- [x] Treinar modelo Random Forest com otimiza√ß√£o autom√°tica\n",
        "- [x] Avaliar performance com valida√ß√£o cruzada\n",
        "- [x] Criar sistema preditivo para novas cr√≠ticas\n",
        "\n",
        "## üìä Dataset\n",
        "\n",
        "- **Fonte**: Dataset IMDB Reviews em Portugu√™s\n",
        "- **Total de cr√≠ticas**: 49,459\n",
        "- **Distribui√ß√£o balanceada**:\n",
        "  - Negativas (neg): 24,765\n",
        "  - Positivas (pos): 24,694\n",
        "- **Colunas dispon√≠veis**: `id`, `text_en`, `text_pt`, `sentiment`\n",
        "\n",
        "## üèóÔ∏è Arquitetura do Sistema\n",
        "\n",
        "### 1. **Pr√©-processamento de Texto**\n",
        "```python\n",
        "Etapas do pr√©-processamento:\n",
        "1. Convers√£o para min√∫sculas\n",
        "2. Remo√ß√£o de tags HTML\n",
        "3. Filtro de caracteres especiais\n",
        "4. Tokeniza√ß√£o em portugu√™s\n",
        "5. Remo√ß√£o de stopwords\n",
        "6. Stemming (redu√ß√£o √† raiz)\n",
        "7. Reconstru√ß√£o do texto"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a23e68f7",
      "metadata": {},
      "source": [
        "### 3. **Modelo de Classifica√ß√£o**\n",
        "- **Algoritmo**: Random Forest Classifier\n",
        "- **Vantagens**:\n",
        "  - Modelo ensemble (m√∫ltiplas √°rvores)\n",
        "  - Menos propenso a overfitting\n",
        "  - Lida bem com muitas features\n",
        "- **Hiperpar√¢metros otimizados** via GridSearchCV\n",
        "\n",
        "\n",
        "### 4. **Otimiza√ß√£o Autom√°tica**\n",
        "```python\n",
        "GridSearchCV com:\n",
        "- Valida√ß√£o cruzada: 3 folds\n",
        "- M√©trica: Acur√°cia\n",
        "- Teste de m√∫ltiplos par√¢metros\n",
        "- Paraleliza√ß√£o completa\n",
        "```\n",
        "\n",
        "### 2. **Vetoriza√ß√£o TF-IDF**\n",
        "- Considera frequ√™ncia da palavra no documento\n",
        "- Penaliza palavras muito comuns\n",
        "- Captura import√¢ncia relativa das palavras\n",
        "- Configura√ß√µes otimizadas:\n",
        "  - `max_features=5000`\n",
        "  - `ngram_range=(1,2)`\n",
        "  - `min_df=5`\n",
        "  - `max_df=0.7`\n",
        "\n",
        "\n",
        "\n",
        "### 4. **Otimiza√ß√£o Autom√°tica**\n",
        "```python\n",
        "GridSearchCV com:\n",
        "- Valida√ß√£o cruzada: 3 folds\n",
        "- M√©trica: Acur√°cia\n",
        "- Teste de m√∫ltiplos par√¢metros\n",
        "- Paraleliza√ß√£o completa\n",
        "```\n",
        "\n",
        "## üìà Resultados Esperados\n",
        "\n",
        "| M√©trica | Valor Esperado |\n",
        "|---------|---------------|\n",
        "| Acur√°cia | 80-90% |\n",
        "| Precis√£o | > 85% |\n",
        "| Recall | > 85% |\n",
        "| F1-Score | > 85% |\n",
        "\n",
        "## üîß Instala√ß√£o e Execu√ß√£o\n",
        "\n",
        "### 1. Pr√©-requisitos\n",
        "```bash\n",
        "# Vers√£o do Python\n",
        "Python 3.8 ou superior\n",
        "\n",
        "# Instalar depend√™ncias\n",
        "pip install pandas numpy scikit-learn nltk\n",
        "\n",
        "# Baixar recursos do NLTK\n",
        "python -c \"import nltk; nltk.download('punkt_tab'); nltk.download('punkt'); nltk.download('stopwords')\"\n",
        "```\n",
        "\n",
        "### 2. Estrutura do Projeto\n",
        "```\n",
        "analise-sentimentos/\n",
        "‚îú‚îÄ‚îÄ AnaliseDeSentimentos.ipynb    # Notebook principal\n",
        "‚îú‚îÄ‚îÄ imdb-reviews-pt-br.csv       # Dataset\n",
        "‚îú‚îÄ‚îÄ README.md                    # Documenta√ß√£o\n",
        "‚îî‚îÄ‚îÄ requirements.txt            # Depend√™ncias\n",
        "```\n",
        "\n",
        "### 3. Execu√ß√£o\n",
        "```bash\n",
        "# Executar o notebook completo\n",
        "jupyter notebook AnaliseDeSentimentos.ipynb\n",
        "\n",
        "# Ou executar como script Python\n",
        "python AnaliseDeSentimentos.py\n",
        "```\n",
        "\n",
        "## üöÄ Como Usar o Modelo\n",
        "\n",
        "```python\n",
        "from seu_modelo import analisar_sentimento\n",
        "\n",
        "# Exemplos de uso\n",
        "criticas = [\n",
        "    \"Filme incr√≠vel! Atua√ß√µes impec√°veis.\",\n",
        "    \"Perda de tempo total, n√£o recomendo.\",\n",
        "    \"Razo√°vel, poderia ser melhor.\"\n",
        "]\n",
        "\n",
        "for critica in criticas:\n",
        "    resultado = analisar_sentimento(critica)\n",
        "    print(f\"Cr√≠tica: {critica[:50]}...\")\n",
        "    print(f\"Sentimento: {resultado['sentimento']}\")\n",
        "    print(f\"Confian√ßa: {resultado['confianca']:.2%}\")\n",
        "```\n",
        "\n",
        "## üìÅ Estrutura do C√≥digo\n",
        "\n",
        "### M√≥dulos Principais\n",
        "\n",
        "1. **`preprocessamento_avancado()`**\n",
        "   - Fun√ß√£o principal de limpeza de texto\n",
        "   - Suporte a caracteres acentuados em portugu√™s\n",
        "   - Remo√ß√£o inteligente de stopwords\n",
        "\n",
        "2. **`Pipeline` de Machine Learning**\n",
        "   - Integra√ß√£o TF-IDF + Random Forest\n",
        "   - Encapsulamento completo do fluxo\n",
        "   - Facilidade de manuten√ß√£o\n",
        "\n",
        "3. **`GridSearchCV`**\n",
        "   - Busca exaustiva de melhores par√¢metros\n",
        "   - Valida√ß√£o cruzada incorporada\n",
        "   - Paraleliza√ß√£o para performance\n",
        "\n",
        "### Fluxo de Execu√ß√£o\n",
        "```\n",
        "Carregar Dados ‚Üí Pr√©-processar ‚Üí Vetorizar ‚Üí Treinar ‚Üí Otimizar ‚Üí Avaliar ‚Üí Predizer\n",
        "```\n",
        "\n",
        "## üé® Features Implementadas\n",
        "\n",
        "### ‚úÖ Corrigidas do C√≥digo Original\n",
        "- **Pr√©-processamento**: Mant√©m palavras inteiras (n√£o letras soltas)\n",
        "- **Tokeniza√ß√£o**: Usa `punkt_tab` para portugu√™s\n",
        "- **Vetoriza√ß√£o**: TF-IDF em vez de CountVectorizer simples\n",
        "- **Modelo**: Random Forest em vez de Naive Bayes b√°sico\n",
        "\n",
        "### ‚úÖ Otimiza√ß√µes Adicionais\n",
        "- Pipeline organizado com Scikit-learn\n",
        "- Otimiza√ß√£o autom√°tica de hiperpar√¢metros\n",
        "- Valida√ß√£o cruzada para avalia√ß√£o robusta\n",
        "- An√°lise detalhada de erros\n",
        "\n",
        "## üìä An√°lise de Desempenho\n",
        "\n",
        "### M√©tricas de Avalia√ß√£o\n",
        "- **Acur√°cia**: Porcentagem de classifica√ß√µes corretas\n",
        "- **Precis√£o**: Entre as classificadas como positivas, quantas realmente s√£o\n",
        "- **Recall**: Entre todas as positivas reais, quantas foram identificadas\n",
        "- **F1-Score**: M√©dia harm√¥nica entre precis√£o e recall\n",
        "\n",
        "### Matriz de Confus√£o\n",
        "```\n",
        "              Predito Negativo  Predito Positivo\n",
        "Real Negativo      TN                FP\n",
        "Real Positivo      FN                TP\n",
        "```\n",
        "\n",
        "## üîÑ Pr√≥ximas Melhorias\n",
        "\n",
        "### 1. Engenharia de Features Avan√ßada\n",
        "- [ ] Contagem de palavras positivas/negativas\n",
        "- [ ] Extra√ß√£o de emoticons e exclama√ß√µes\n",
        "- [ ] An√°lise de senten√ßas por par√°grafo\n",
        "\n",
        "### 2. Modelos Avan√ßados\n",
        "- [ ] XGBoost ou LightGBM\n",
        "- [ ] SVM com kernel n√£o-linear\n",
        "- [ ] Redes Neurais (MLP)\n",
        "\n",
        "### 3. Deep Learning\n",
        "- [ ] LSTM/GRU para contexto sequencial\n",
        "- [ ] BERTimbau (BERT em portugu√™s)\n",
        "- [ ] Fine-tuning de transformers\n",
        "\n",
        "### 4. Sistema em Produ√ß√£o\n",
        "- [ ] API REST com FastAPI\n",
        "- [ ] Sistema de cache de predi√ß√µes\n",
        "- [ ] Monitoramento de performance\n",
        "- [ ] Logs detalhados\n",
        "\n",
        "## üìù Conclus√£o\n",
        "\n",
        "Este projeto demonstra uma implementa√ß√£o completa de an√°lise de sentimentos, abordando desde o pr√©-processamento b√°sico at√© otimiza√ß√µes avan√ßadas. A arquitetura modular permite f√°cil extens√£o e adapta√ß√£o para diferentes dom√≠nios.\n",
        "\n",
        "### Principais Aprendizados\n",
        "1. **Pr√©-processamento √© crucial**: Representa√ß√£o correta dos dados afeta diretamente os resultados\n",
        "2. **TF-IDF > CountVectorizer**: Considera import√¢ncia relativa das palavras\n",
        "3. **Random Forest robusto**: Excelente para problemas de classifica√ß√£o de texto\n",
        "4. **Otimiza√ß√£o sistem√°tica**: GridSearchCV encontra automaticamente os melhores par√¢metros\n",
        "\n",
        "## üë• Contribui√ß√£o\n",
        "\n",
        "Contribui√ß√µes s√£o bem-vindas! Siga estes passos:\n",
        "\n",
        "1. Fork do reposit√≥rio\n",
        "2. Crie uma branch (`git checkout -b feature/nova-feature`)\n",
        "3. Commit suas mudan√ßas (`git commit -m 'Add nova feature'`)\n",
        "4. Push para a branch (`git push origin feature/nova-feature`)\n",
        "5. Abra um Pull Request\n",
        "\n",
        "## üìÑ Licen√ßa\n",
        "\n",
        "Este projeto est√° sob a licen√ßa MIT. Veja o arquivo [LICENSE](LICENSE) para detalhes.\n",
        "\n",
        "## üôè Agradecimentos\n",
        "\n",
        "- Dataset: [IMDB Reviews em Portugu√™s](https://www.kaggle.com/datasets)\n",
        "- Bibliotecas: Scikit-learn, NLTK, Pandas, NumPy\n",
        "- Comunidade de Data Science\n",
        "\n",
        "## üìû Contato\n",
        "\n",
        "Para d√∫vidas ou sugest√µes, entre em contato:\n",
        "\n",
        "**Desenvolvedor**: [Seu Nome]  \n",
        "**Email**: seu.email@exemplo.com  \n",
        "**LinkedIn**: [linkedin.com/in/seu-perfil](https://linkedin.com)\n",
        "\n",
        "---\n",
        "*\"Transformando texto em insights atrav√©s de dados\"* üöÄ\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **PRINCIPAIS CORRE√á√ïES APLICADAS:**\n",
        "\n",
        "1. **Corrigido erro do NLTK**: Adicionado download do `punkt_tab`\n",
        "2. **Sequ√™ncia l√≥gica**: Garantida execu√ß√£o na ordem correta\n",
        "3. **Simplifica√ß√£o**: Reduzida complexidade do GridSearchCV para execu√ß√£o mais r√°pida\n",
        "4. **Manuten√ß√£o de contexto**: Todas as vari√°veis s√£o definidas antes do uso\n",
        "\n",
        "## **PR√ìXIMOS PASSOS SUGERIDOS:**\n",
        "\n",
        "1. **Salvar o modelo treinado**:\n",
        "```python\n",
        "import joblib\n",
        "joblib.dump(grid_search, 'modelo_sentimentos.pkl')\n",
        "```\n",
        "\n",
        "2. **Criar API**:\n",
        "```python\n",
        "from fastapi import FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/analisar\")\n",
        "def analisar(critica: str):\n",
        "    texto_limpo = preprocessamento_avancado(critica)\n",
        "    predicao = grid_search.predict([texto_limpo])[0]\n",
        "    return {\"sentimento\": \"positivo\" if predicao == 1 else \"negativo\"}\n",
        "```\n",
        "\n",
        "3. **Monitoramento**:\n",
        "   - Adicionar logging\n",
        "   - Implementar tracking de performance\n",
        "   - Criar dashboard de m√©tricas\n",
        "\n",
        "O projeto est√° agora funcional e pronto para execu√ß√£o!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
