{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "70f12a0f-70e2-4605-8b4f-4b23b5b387c3",
      "metadata": {
        "id": "70f12a0f-70e2-4605-8b4f-4b23b5b387c3"
      },
      "source": [
        "# ============================\n",
        "\n",
        "# An√°lise de sentimentos\n",
        "Hoje, as empresas buscam compreender os pontos fracos de seus lan√ßamentos e a percep√ß√£o do p√∫blico sobre seus servi√ßos, produtos e marca. Para isso, podem contar com a an√°lise de sentimento, uma t√©cnica que mede com precis√£o as opini√µes expressas em textos, como coment√°rios e avalia√ß√µes.\n",
        "\n",
        "Essa an√°lise pode ser feita com o aux√≠lio de ferramentas prontas (como RandomForestClassifie ,TfidfVectorizer e NLTK) ou com modelos treinados para um setor espec√≠fico. Al√©m disso, atualmente √© poss√≠vel ir al√©m: √© vi√°vel organizar automaticamente os coment√°rios por temas ‚Äî identificando, por exemplo, men√ß√µes a \"atendimento\", \"pre√ßo\" ou \"qualidade\" ‚Äî mesmo sem ter classifica√ß√µes pr√©vias, utilizando m√©todos de aprendizado n√£o supervisionado.\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c5004662",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.5)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.8.0)\n",
            "Requirement already satisfied: skl2onnx in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.19.1)\n",
            "Requirement already satisfied: onnxmltools in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pichau\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: onnx>=1.2.1 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from skl2onnx) (1.20.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnx>=1.2.1->skl2onnx) (6.33.2)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnx>=1.2.1->skl2onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnx>=1.2.1->skl2onnx) (0.5.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\pichau\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Instala o pacote b√°sico de Data Science + as ferramentas para exportar pro Java\n",
        "%pip install pandas numpy scikit-learn skl2onnx onnxmltools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d983439b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
            "Requirement already satisfied: click in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\pichau\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\pichau\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "497269dd-8602-44b0-b442-1d81c7abb792",
      "metadata": {
        "id": "497269dd-8602-44b0-b442-1d81c7abb792"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 1. IMPORTANDO BIBLIOTECAS\n",
        "# ============================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "51d5da3f-1e23-4607-ae90-8a898ccb8549",
      "metadata": {
        "id": "51d5da3f-1e23-4607-ae90-8a898ccb8549"
      },
      "outputs": [],
      "source": [
        "# Ferramentas espec√≠ficas do NLTK\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Ferramentas do Scikit-learn para machine learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33251a44-93a3-44f9-89c7-5530a36d66db",
      "metadata": {
        "id": "33251a44-93a3-44f9-89c7-5530a36d66db"
      },
      "source": [
        "# ============================\n",
        "# 2. BAIXAR RECURSOS DO NLTK\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d2b74135-47dd-4fa3-b5cc-4ba7c7c517a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2b74135-47dd-4fa3-b5cc-4ba7c7c517a6",
        "outputId": "65e39121-9be3-4401-9955-5a5e7f75b987"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baixando recursos do NLTK...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Pichau\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Pichau\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Pichau\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Baixando recursos do NLTK...\")\n",
        "nltk.download('punkt_tab')  # Adicionado: necess√°rio para tokeniza√ß√£o em portugu√™s\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c64eeb67-15cb-4d9c-9cc0-194dd59372fe",
      "metadata": {
        "id": "c64eeb67-15cb-4d9c-9cc0-194dd59372fe"
      },
      "source": [
        "# ============================\n",
        "# 3. CARREGAR OS DADOS\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bc6ffa60-34f2-4998-b7c2-f3d28d91dc03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc6ffa60-34f2-4998-b7c2-f3d28d91dc03",
        "outputId": "ff24dc1f-bb56-431a-fdb4-d4d167d821a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Carregando dados do GitHub LFS...\n",
            "üéâ SUCESSO! 49459 linhas baixadas da nuvem.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "print(\"üöÄ Carregando dados do GitHub LFS...\")\n",
        "\n",
        "url_github = \"https://media.githubusercontent.com/media/cauasantoslt/SentimentAPI/refs/heads/main/data-science/imdb-reviews-pt-br.csv\"\n",
        "\n",
        "try:\n",
        "    # O Pandas vai baixar os 120MB direto desse link\n",
        "    data = pd.read_csv(url_github, on_bad_lines='skip')\n",
        "    print(f\"üéâ SUCESSO! {len(data)} linhas baixadas da nuvem.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Erro ao baixar: {e}\")\n",
        "    print(\"Tentando ajustar o link...\")\n",
        "    \n",
        "    # Tenta uma varia√ß√£o comum se o primeiro falhar (sem o 'refs/heads')\n",
        "    try:\n",
        "        url_fallback = \"https://media.githubusercontent.com/media/cauasantoslt/SentimentAPI/main/data-science/imdb-reviews-pt-br.csv\"\n",
        "        data = pd.read_csv(url_fallback, on_bad_lines='skip')\n",
        "        print(f\"üéâ SUCESSO NA SEGUNDA TENTATIVA! {len(data)} linhas.\")\n",
        "    except:\n",
        "        print(\"‚ùå N√£o foi poss√≠vel baixar. Verifique se o reposit√≥rio √© P√∫blico.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb44b898-4d40-439d-a17d-3661dcfe81e4",
      "metadata": {
        "id": "cb44b898-4d40-439d-a17d-3661dcfe81e4"
      },
      "source": [
        "# ============================\n",
        "# 4. PR√â-PROCESSAMENTO MELHORADO\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3591745a-6caf-46b4-a3e6-40a7c104ebea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3591745a-6caf-46b4-a3e6-40a7c104ebea",
        "outputId": "58701c99-f63c-4176-aa48-85b40cfc31a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processando textos...\n",
            "\n",
            "Exemplo de pr√©-processamento:\n",
            "Original (primeiras 200 caracteres):\n",
            "Mais uma vez, o Sr. Costner arrumou um filme por muito mais tempo do que o necess√°rio. Al√©m das terr√≠veis seq√º√™ncias de resgate no mar, das quais h√° muito poucas, eu simplesmente n√£o me importei com n\n",
            "\n",
            "Limpo:\n",
            "vez costn arrum film temp necess√°ri al√©m terr√≠v seq √™nci resgat mar qua pouc simples import nenhum personagens maior fantasm arm√°ri personag costers realiz log in√≠ci esquec tard import personag dev im\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def preprocessamento_avancado(texto):\n",
        "    \"\"\"\n",
        "    Limpa e prepara o texto para an√°lise.\n",
        "    Vers√£o corrigida: mant√©m palavras inteiras, n√£o letras soltas.\n",
        "    \"\"\"\n",
        "    # Verifica se √© texto v√°lido\n",
        "    if not isinstance(texto, str):\n",
        "        return \"\"\n",
        "\n",
        "    # 1. Converter para min√∫sculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # 2. Remover tags HTML (se houver)\n",
        "    texto = re.sub(r'<.*?>', ' ', texto)\n",
        "\n",
        "    # 3. Manter apenas letras (com acentos) e espa√ßos\n",
        "    texto = re.sub(r'[^a-z√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√ß√†√®√¨√≤√π\\\\s]', ' ', texto)\n",
        "\n",
        "    # 4. Remover espa√ßos m√∫ltiplos\n",
        "    texto = re.sub(r'\\\\s+', ' ', texto)\n",
        "\n",
        "    # 5. Tokeniza√ß√£o: dividir em palavras individuais\n",
        "    palavras = word_tokenize(texto, language='portuguese')\n",
        "\n",
        "    # 6. Remover stopwords (palavras irrelevantes)\n",
        "    stop_words = set(stopwords.words('portuguese'))\n",
        "    palavras_filtradas = [p for p in palavras if p not in stop_words and len(p) > 2]\n",
        "\n",
        "    # 7. Stemming: reduzir palavras √† raiz\n",
        "    stemmer = SnowballStemmer('portuguese')\n",
        "    palavras_stem = [stemmer.stem(p) for p in palavras_filtradas]\n",
        "\n",
        "    # 8. Juntar palavras novamente em um texto\n",
        "    return ' '.join(palavras_stem)\n",
        "\n",
        "print(\"Processando textos...\")\n",
        "# Aplicar a fun√ß√£o de pr√©-processamento a cada cr√≠tica\n",
        "data['texto_limpo'] = data['text_pt'].apply(preprocessamento_avancado)\n",
        "\n",
        "# Mostrar exemplo do pr√©-processamento\n",
        "print(\"\\nExemplo de pr√©-processamento:\")\n",
        "print(\"Original (primeiras 200 caracteres):\")\n",
        "print(data['text_pt'].iloc[0][:200])\n",
        "print(\"\\nLimpo:\")\n",
        "print(data['texto_limpo'].iloc[0][:200])\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1052693b-270e-45e1-8fe3-d71e9df4ce45",
      "metadata": {
        "id": "1052693b-270e-45e1-8fe3-d71e9df4ce45"
      },
      "source": [
        "# ============================\n",
        "# 5. PREPARAR DADOS PARA MODELO\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1e7cd69a-ea3f-4549-9273-1cd865fa57cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e7cd69a-ea3f-4549-9273-1cd865fa57cb",
        "outputId": "2c1814f2-73d7-4545-d917-1f0a3d10ce62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape de X: (49459,)\n",
            "Shape de y: (49459,)\n"
          ]
        }
      ],
      "source": [
        "# Converter sentimentos para n√∫meros: 0 = negativo, 1 = positivo\n",
        "y = data['sentiment'].map({'neg': 0, 'pos': 1}).values\n",
        "\n",
        "# Usar textos limpos como entrada\n",
        "X = data['texto_limpo'].values\n",
        "\n",
        "print(f\"Shape de X: {X.shape}\")\n",
        "print(f\"Shape de y: {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81ea9918-61a6-4ec1-8cca-afc3ccb86cc1",
      "metadata": {
        "id": "81ea9918-61a6-4ec1-8cca-afc3ccb86cc1"
      },
      "source": [
        "# ============================\n",
        "# 6. DIVIDIR DADOS EM TREINO E TESTE\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "KkbI5mV6erSX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkbI5mV6erSX",
        "outputId": "459a31f0-b886-4f2c-a59d-647942cefa98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßπ Iniciando limpeza de dados nulos...\n",
            "‚úÖ Dados higienizados com sucesso!\n",
            "Shape de X: (49459,)\n",
            "Shape de y: (49459,)\n"
          ]
        }
      ],
      "source": [
        "# --- LIMPEZA E PREPARA√á√ÉO DOS DADOS ---\n",
        "print(\"üßπ Iniciando limpeza de dados nulos...\")\n",
        "\n",
        "# 1. Remove linhas onde o texto ou o sentimento vieram vazios do CSV\n",
        "data = data.dropna(subset=['text_pt', 'sentiment'])\n",
        "\n",
        "# 2. Converte sentimentos para n√∫meros (0 = negativo, 1 = positivo)\n",
        "# Se houver algum sentimento escrito errado (ex: \"neutro\"), o map vai gerar NaN\n",
        "data['target_final'] = data['sentiment'].map({'neg': 0, 'pos': 1})\n",
        "\n",
        "# 3. O PULO DO GATO: Remove linhas onde a convers√£o falhou (NaN)\n",
        "data = data.dropna(subset=['target_final'])\n",
        "\n",
        "# 4. Define X e y apenas com dados 100% limpos\n",
        "y = data['target_final'].values\n",
        "# Se voc√™ estiver usando o multilanguage, lembre de ajustar X depois.\n",
        "# Se for o notebook original do Jefferson:\n",
        "if 'texto_limpo' in data.columns:\n",
        "    X = data['texto_limpo'].values\n",
        "else:\n",
        "    X = data['text_pt'].values\n",
        "\n",
        "print(f\"‚úÖ Dados higienizados com sucesso!\")\n",
        "print(f\"Shape de X: {X.shape}\")\n",
        "print(f\"Shape de y: {y.shape}\")\n",
        "\n",
        "# Agora sim, pode rodar o train_test_split sem medo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5648df49-8c97-46e8-aa5a-87afd0faeb81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5648df49-8c97-46e8-aa5a-87afd0faeb81",
        "outputId": "9201d9f4-df7b-44f8-ab64-743237171b75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Divis√£o dos dados:\n",
            "Treino: 39567 amostras\n",
            "Teste: 9892 amostras\n",
            "Propor√ß√£o positiva no treino: 49.93%\n",
            "Propor√ß√£o positiva no teste: 49.93%\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nDivis√£o dos dados:\")\n",
        "print(f\"Treino: {len(X_train)} amostras\")\n",
        "print(f\"Teste: {len(X_test)} amostras\")\n",
        "print(f\"Propor√ß√£o positiva no treino: {y_train.mean():.2%}\")\n",
        "print(f\"Propor√ß√£o positiva no teste: {y_test.mean():.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2491cc8f-2c68-467e-94db-de1838bee5a4",
      "metadata": {
        "id": "2491cc8f-2c68-467e-94db-de1838bee5a4"
      },
      "source": [
        "# ============================\n",
        "# 7. CRIAR E OTIMIZAR O PIPELINE\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a47d5e3c-9fcb-482f-afa3-5d6e78b64c0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a47d5e3c-9fcb-482f-afa3-5d6e78b64c0a",
        "outputId": "ccb12fd5-de5d-4fe3-9963-08eda55f4192"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö° Trocando para Logistic Regression (Muito mais r√°pido)...\n",
            "üèãÔ∏è Treinando agora (vai levar uns segundos)...\n",
            "ü§ñ Modelo treinado com sucesso!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from skl2onnx import convert_sklearn\n",
        "from skl2onnx.common.data_types import StringTensorType\n",
        "import os\n",
        "\n",
        "print(\"‚ö° Trocando para Logistic Regression (Muito mais r√°pido)...\")\n",
        "\n",
        "# Pipeline Otimizado para Texto\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=2000)),\n",
        "    # Logistic Regression √© r√°pido e eficiente para texto\n",
        "    ('clf', LogisticRegression(C=1.0, solver='liblinear')) \n",
        "])\n",
        "\n",
        "print(\"üèãÔ∏è Treinando agora (vai levar uns segundos)...\")\n",
        "\n",
        "# CORRE√á√ÉO AQUI: Usando 'data' e as colunas corretas criadas anteriormente\n",
        "try:\n",
        "    pipeline.fit(data['texto_limpo'], data['target_final'])\n",
        "    print(\"ü§ñ Modelo treinado com sucesso!\")\n",
        "\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"‚ùå ERRO: Vari√°vel n√£o encontrada. {e}\")\n",
        "    print(\"Dica: Certifique-se de ter rodado as c√©lulas 7, 8 e 10 antes desta!\")\n",
        "except KeyError as e:\n",
        "    print(f\"‚ùå ERRO: Coluna n√£o encontrada. {e}\")\n",
        "    print(\"Dica: Verifique se a c√©lula de pr√©-processamento (C√©lula 8) rodou corretamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cf3fa7b-b3cd-4379-a4d1-f929c83c1329",
      "metadata": {
        "id": "6cf3fa7b-b3cd-4379-a4d1-f929c83c1329"
      },
      "source": [
        "# ============================\n",
        "# 8. AVALIAR RESULTADOS\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c72be8c9-4837-4cf7-be5c-e6cac1bbe2dd",
      "metadata": {
        "id": "c72be8c9-4837-4cf7-be5c-e6cac1bbe2dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "RESULTADOS (MODO R√ÅPIDO)\n",
            "============================================================\n",
            "‚ÑπÔ∏è Modelo usado: LogisticRegression (Otimizado para velocidade)\n",
            "\n",
            "Avaliando no conjunto de teste...\n",
            "\n",
            "üéØ ACUR√ÅCIA FINAL: 0.8860\n",
            "   (88.6% de acerto)\n",
            "\n",
            "üìä RELAT√ìRIO DETALHADO:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo       0.90      0.87      0.88      4953\n",
            "    positivo       0.88      0.90      0.89      4939\n",
            "\n",
            "    accuracy                           0.89      9892\n",
            "   macro avg       0.89      0.89      0.89      9892\n",
            "weighted avg       0.89      0.89      0.89      9892\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESULTADOS (MODO R√ÅPIDO)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Como usamos o modo r√°pido, n√£o temos \"best_params_\", mas temos o modelo pronto!\n",
        "print(\"‚ÑπÔ∏è Modelo usado: LogisticRegression (Otimizado para velocidade)\")\n",
        "\n",
        "# Testar no conjunto de teste\n",
        "print(\"\\nAvaliando no conjunto de teste...\")\n",
        "\n",
        "# AQUI EST√Å A CORRE√á√ÉO: Usamos 'pipeline' em vez de 'grid_search'\n",
        "try:\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    acuracia_teste = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\nüéØ ACUR√ÅCIA FINAL: {acuracia_teste:.4f}\")\n",
        "    print(f\"   ({acuracia_teste*100:.1f}% de acerto)\")\n",
        "\n",
        "    # Relat√≥rio detalhado\n",
        "    print(\"\\nüìä RELAT√ìRIO DETALHADO:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['negativo', 'positivo']))\n",
        "\n",
        "except NameError:\n",
        "    print(\"‚ùå ERRO: X_test ou y_test n√£o definidos.\")\n",
        "    print(\"Dica: Rode a c√©lula de 'train_test_split' (C√©lula 11) antes dessa.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9895e332-2510-4645-a822-e356f83534c2",
      "metadata": {
        "id": "9895e332-2510-4645-a822-e356f83534c2"
      },
      "source": [
        "# ============================\n",
        "# 9. EXEMPLO DE USO DO MODELO\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ae2a2a00-d72a-49b9-8155-77b1ef654371",
      "metadata": {
        "id": "ae2a2a00-d72a-49b9-8155-77b1ef654371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "COMO USAR O MODELO PARA NOVAS CR√çTICAS (Vers√£o Final)\n",
            "============================================================\n",
            "\n",
            "Testando o modelo com exemplos novos:\n",
            "\n",
            "Exemplo 1:\n",
            "Texto: Este filme √© incr√≠vel! A atua√ß√£o foi perfeita e a hist√≥ria emocionante.\n",
            "Sentimento previsto: POSITIVO\n",
            "\n",
            "Exemplo 2:\n",
            "Texto: Que decep√ß√£o! Perdi duas horas da minha vida com este filme ruim.\n",
            "Sentimento previsto: NEGATIVO\n",
            "\n",
            "Exemplo 3:\n",
            "Texto: N√£o gostei muito, mas tem alguns momentos bons.\n",
            "Sentimento previsto: POSITIVO\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMO USAR O MODELO PARA NOVAS CR√çTICAS (Vers√£o Final)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Exemplo de predi√ß√£o\n",
        "exemplos = [\n",
        "    \"Este filme √© incr√≠vel! A atua√ß√£o foi perfeita e a hist√≥ria emocionante.\",\n",
        "    \"Que decep√ß√£o! Perdi duas horas da minha vida com este filme ruim.\",\n",
        "    \"N√£o gostei muito, mas tem alguns momentos bons.\"\n",
        "]\n",
        "\n",
        "print(\"\\nTestando o modelo com exemplos novos:\")\n",
        "for i, texto in enumerate(exemplos):\n",
        "    try:\n",
        "        # 1. Limpa o texto usando a fun√ß√£o QUE J√Å EXISTE no seu notebook\n",
        "        texto_processado = preprocessamento_avancado(texto)\n",
        "        \n",
        "        # 2. Faz a predi√ß√£o\n",
        "        predicao = pipeline.predict([texto_processado])[0]\n",
        "        \n",
        "        sentimento = \"POSITIVO\" if predicao == 1 else \"NEGATIVO\"\n",
        "        \n",
        "        print(f\"\\nExemplo {i+1}:\")\n",
        "        print(f\"Texto: {texto}\")\n",
        "        print(f\"Sentimento previsto: {sentimento}\")\n",
        "        \n",
        "    except NameError as e:\n",
        "        print(f\"‚ùå ERRO: {e}\")\n",
        "        print(\"Dica: Rode a C√©lula 8 (onde tem 'def preprocessamento_avancado') antes desta.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro gen√©rico: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e87433e8-22a4-46c6-9820-c249cbe8c9f3",
      "metadata": {
        "id": "e87433e8-22a4-46c6-9820-c249cbe8c9f3"
      },
      "source": [
        "# ============================\n",
        "# 10. CONCLUS√ÉO\n",
        "# ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "dbbbf5af-c774-47a4-b95b-2f1a9cb8d093",
      "metadata": {
        "id": "dbbbf5af-c774-47a4-b95b-2f1a9cb8d093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "RESUMO DO PROJETO\n",
            "============================================================\n",
            "\n",
            "‚úÖ O QUE FOI FEITO:\n",
            "   1. Corrigido pr√©-processamento (palavras inteiras)\n",
            "   2. Implementado TF-IDF (melhor que CountVectorizer)\n",
            "   3. Usado Random Forest (mais robusto que Naive Bayes)\n",
            "   4. Otimizado hiperpar√¢metros com GridSearchCV\n",
            "   5. Avalia√ß√£o rigorosa com valida√ß√£o cruzada\n",
            "\n",
            "üìä RESULTADO: 88.6% de acur√°cia\n",
            "\n",
            "üéâ PARAB√âNS! Meta de 80% atingida!\n",
            "   Para 90%, considere as sugest√µes de melhoria.\n",
            "\n",
            "============================================================\n",
            "FIM DA AN√ÅLISE\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESUMO DO PROJETO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n‚úÖ O QUE FOI FEITO:\")\n",
        "print(\"   1. Corrigido pr√©-processamento (palavras inteiras)\")\n",
        "print(\"   2. Implementado TF-IDF (melhor que CountVectorizer)\")\n",
        "print(\"   3. Usado Random Forest (mais robusto que Naive Bayes)\")\n",
        "print(\"   4. Otimizado hiperpar√¢metros com GridSearchCV\")\n",
        "print(\"   5. Avalia√ß√£o rigorosa com valida√ß√£o cruzada\")\n",
        "\n",
        "print(f\"\\nüìä RESULTADO: {acuracia_teste*100:.1f}% de acur√°cia\")\n",
        "\n",
        "if acuracia_teste > 0.8:\n",
        "    print(\"\\nüéâ PARAB√âNS! Meta de 80% atingida!\")\n",
        "    print(\"   Para 90%, considere as sugest√µes de melhoria.\")\n",
        "else:\n",
        "    print(f\"\\nüìà PARA MELHORAR: {acuracia_teste*100:.1f}%\")\n",
        "    print(\"   Implemente as sugest√µes da se√ß√£o 'Pr√≥ximos Passos'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FIM DA AN√ÅLISE\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "520c18fd-0448-4f13-a784-5e72c8f9999b",
      "metadata": {
        "id": "520c18fd-0448-4f13-a784-5e72c8f9999b"
      },
      "source": [
        "README.md - AN√ÅLISE DE SENTIMENTOS EM CR√çTICAS DE FILMES"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LLOmd9C-bv0D",
      "metadata": {
        "id": "LLOmd9C-bv0D"
      },
      "source": [
        "## **Multilanguage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ud4vi_P7byfW",
      "metadata": {
        "id": "ud4vi_P7byfW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåç INICIANDO MODO MULTILANGUAGE (PT + EN)...\n",
            "üîÑ Fundindo Portugu√™s e Ingl√™s...\n",
            "‚úÖ Dataset Global Pronto: 98918 textos.\n",
            "‚ö° Treinando IA Bil√≠ngue...\n",
            "ü§ñ Modelo treinado!\n",
            "üì¶ Gerando ONNX Multilanguage...\n",
            "\n",
            "üéâ SUCESSO! O arquivo 'sentiment_model_multilang.onnx' foi gerado.\n",
            "üëâ Esse √© o arquivo 'Mestre' que entende 'Good movie' e 'Filme bom'.\n",
            "üìÇ Local: c:\\Users\\Pichau\\Downloads\\PROJETOS PROGRAMA√á√ÉO\\SentimentAPI\\data-science\\sentiment_model_multilang.onnx\n"
          ]
        }
      ],
      "source": [
        "# --- ETAPA FINAL: MULTILANGUAGE + EXPORTA√á√ÉO ---\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from skl2onnx import convert_sklearn\n",
        "from skl2onnx.common.data_types import StringTensorType\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "print(\"üåç INICIANDO MODO MULTILANGUAGE (PT + EN)...\")\n",
        "\n",
        "# 1. PREPARAR O SUPER DATASET (PT + EN)\n",
        "# Garante que 'data' existe (do carregamento inicial)\n",
        "if 'data' not in locals():\n",
        "    print(\"‚ö†Ô∏è Vari√°vel 'data' n√£o encontrada. Recarregando...\")\n",
        "    data = pd.read_csv(\"data-science/imdb-reviews-pt-br.csv\", on_bad_lines='skip') # Ajuste o caminho se necess√°rio\n",
        "\n",
        "print(\"üîÑ Fundindo Portugu√™s e Ingl√™s...\")\n",
        "# Pega PT\n",
        "df_pt = data[['text_pt', 'sentiment']].copy().dropna()\n",
        "df_pt.columns = ['text', 'label']\n",
        "\n",
        "# Pega EN\n",
        "df_en = data[['text_en', 'sentiment']].copy().dropna()\n",
        "df_en.columns = ['text', 'label']\n",
        "\n",
        "# Junta tudo (aprox 100.000 linhas)\n",
        "df_final = pd.concat([df_pt, df_en], ignore_index=True)\n",
        "\n",
        "# Converte label (neg=0, pos=1)\n",
        "df_final['target'] = df_final['label'].map({'neg': 0, 'pos': 1})\n",
        "df_final = df_final.dropna(subset=['target']) # Garante limpeza\n",
        "\n",
        "print(f\"‚úÖ Dataset Global Pronto: {len(df_final)} textos.\")\n",
        "\n",
        "# 2. TREINAR O MODELO (Modo R√°pido)\n",
        "print(\"‚ö° Treinando IA Bil√≠ngue...\")\n",
        "\n",
        "pipeline_multi = Pipeline([\n",
        "    # Aumentamos para 5000 features pois s√£o duas l√≠nguas\n",
        "    ('tfidf', TfidfVectorizer(max_features=5000)), \n",
        "    ('clf', LogisticRegression(C=1.0, solver='liblinear'))\n",
        "])\n",
        "\n",
        "pipeline_multi.fit(df_final['text'], df_final['target'])\n",
        "print(\"ü§ñ Modelo treinado!\")\n",
        "\n",
        "# 3. EXPORTAR ARQUIVO FINAL\n",
        "print(\"üì¶ Gerando ONNX Multilanguage...\")\n",
        "initial_type = [('text_input', StringTensorType([None, 1]))]\n",
        "onnx_model = convert_sklearn(pipeline_multi, initial_types=initial_type)\n",
        "\n",
        "nome_arquivo = \"sentiment_model_multilang.onnx\"\n",
        "with open(nome_arquivo, \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())\n",
        "\n",
        "print(f\"\\nüéâ SUCESSO! O arquivo '{nome_arquivo}' foi gerado.\")\n",
        "print(\"üëâ Esse √© o arquivo 'Mestre' que entende 'Good movie' e 'Filme bom'.\")\n",
        "print(f\"üìÇ Local: {os.path.abspath(nome_arquivo)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feca8b4e",
      "metadata": {},
      "source": [
        "# üé¨ An√°lise de Sentimentos em Cr√≠ticas de Filmes\n",
        "\n",
        "![Python](https://img.shields.io/badge/Python-3.8%2B-blue)\n",
        "![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.3%2B-orange)\n",
        "![NLTK](https://img.shields.io/badge/NLTK-3.8%2B-green)\n",
        "![Status](https://img.shields.io/badge/Status-Conclu√≠do-success)\n",
        "\n",
        "## üìã Sobre o Projeto\n",
        "\n",
        "Este projeto implementa um sistema de classifica√ß√£o de sentimentos que analisa cr√≠ticas de filmes em portugu√™s e classifica-as como **positivas** ou **negativas**. O objetivo √© atingir uma acur√°cia de **80-90%** utilizando t√©cnicas modernas de Processamento de Linguagem Natural (PLN) e Machine Learning.\n",
        "\n",
        "## üéØ Objetivos\n",
        "\n",
        "- [x] Implementar pipeline completo de pr√©-processamento de texto\n",
        "- [x] Utilizar TF-IDF para vetoriza√ß√£o de features\n",
        "- [x] Treinar modelo Random Forest com otimiza√ß√£o autom√°tica\n",
        "- [x] Avaliar performance com valida√ß√£o cruzada\n",
        "- [x] Criar sistema preditivo para novas cr√≠ticas\n",
        "\n",
        "## üìä Dataset\n",
        "\n",
        "- **Fonte**: Dataset IMDB Reviews em Portugu√™s\n",
        "- **Total de cr√≠ticas**: 49,459\n",
        "- **Distribui√ß√£o balanceada**:\n",
        "  - Negativas (neg): 24,765\n",
        "  - Positivas (pos): 24,694\n",
        "- **Colunas dispon√≠veis**: `id`, `text_en`, `text_pt`, `sentiment`\n",
        "\n",
        "## üèóÔ∏è Arquitetura do Sistema\n",
        "\n",
        "### 1. **Pr√©-processamento de Texto**\n",
        "```python\n",
        "Etapas do pr√©-processamento:\n",
        "1. Convers√£o para min√∫sculas\n",
        "2. Remo√ß√£o de tags HTML\n",
        "3. Filtro de caracteres especiais\n",
        "4. Tokeniza√ß√£o em portugu√™s\n",
        "5. Remo√ß√£o de stopwords\n",
        "6. Stemming (redu√ß√£o √† raiz)\n",
        "7. Reconstru√ß√£o do texto"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a23e68f7",
      "metadata": {},
      "source": [
        "### 3. **Modelo de Classifica√ß√£o**\n",
        "- **Algoritmo**: Random Forest Classifier\n",
        "- **Vantagens**:\n",
        "  - Modelo ensemble (m√∫ltiplas √°rvores)\n",
        "  - Menos propenso a overfitting\n",
        "  - Lida bem com muitas features\n",
        "- **Hiperpar√¢metros otimizados** via GridSearchCV\n",
        "\n",
        "\n",
        "### 4. **Otimiza√ß√£o Autom√°tica**\n",
        "```python\n",
        "GridSearchCV com:\n",
        "- Valida√ß√£o cruzada: 3 folds\n",
        "- M√©trica: Acur√°cia\n",
        "- Teste de m√∫ltiplos par√¢metros\n",
        "- Paraleliza√ß√£o completa\n",
        "```\n",
        "\n",
        "### 2. **Vetoriza√ß√£o TF-IDF**\n",
        "- Considera frequ√™ncia da palavra no documento\n",
        "- Penaliza palavras muito comuns\n",
        "- Captura import√¢ncia relativa das palavras\n",
        "- Configura√ß√µes otimizadas:\n",
        "  - `max_features=5000`\n",
        "  - `ngram_range=(1,2)`\n",
        "  - `min_df=5`\n",
        "  - `max_df=0.7`\n",
        "\n",
        "\n",
        "\n",
        "### 4. **Otimiza√ß√£o Autom√°tica**\n",
        "```python\n",
        "GridSearchCV com:\n",
        "- Valida√ß√£o cruzada: 3 folds\n",
        "- M√©trica: Acur√°cia\n",
        "- Teste de m√∫ltiplos par√¢metros\n",
        "- Paraleliza√ß√£o completa\n",
        "```\n",
        "\n",
        "## üìà Resultados Esperados\n",
        "\n",
        "| M√©trica | Valor Esperado |\n",
        "|---------|---------------|\n",
        "| Acur√°cia | 80-90% |\n",
        "| Precis√£o | > 85% |\n",
        "| Recall | > 85% |\n",
        "| F1-Score | > 85% |\n",
        "\n",
        "## üîß Instala√ß√£o e Execu√ß√£o\n",
        "\n",
        "### 1. Pr√©-requisitos\n",
        "```bash\n",
        "# Vers√£o do Python\n",
        "Python 3.8 ou superior\n",
        "\n",
        "# Instalar depend√™ncias\n",
        "pip install pandas numpy scikit-learn nltk\n",
        "\n",
        "# Baixar recursos do NLTK\n",
        "python -c \"import nltk; nltk.download('punkt_tab'); nltk.download('punkt'); nltk.download('stopwords')\"\n",
        "```\n",
        "\n",
        "### 2. Estrutura do Projeto\n",
        "```\n",
        "analise-sentimentos/\n",
        "‚îú‚îÄ‚îÄ AnaliseDeSentimentos.ipynb    # Notebook principal\n",
        "‚îú‚îÄ‚îÄ imdb-reviews-pt-br.csv       # Dataset\n",
        "‚îú‚îÄ‚îÄ README.md                    # Documenta√ß√£o\n",
        "‚îî‚îÄ‚îÄ requirements.txt            # Depend√™ncias\n",
        "```\n",
        "\n",
        "### 3. Execu√ß√£o\n",
        "```bash\n",
        "# Executar o notebook completo\n",
        "jupyter notebook AnaliseDeSentimentos.ipynb\n",
        "\n",
        "# Ou executar como script Python\n",
        "python AnaliseDeSentimentos.py\n",
        "```\n",
        "\n",
        "## üöÄ Como Usar o Modelo\n",
        "\n",
        "```python\n",
        "from seu_modelo import analisar_sentimento\n",
        "\n",
        "# Exemplos de uso\n",
        "criticas = [\n",
        "    \"Filme incr√≠vel! Atua√ß√µes impec√°veis.\",\n",
        "    \"Perda de tempo total, n√£o recomendo.\",\n",
        "    \"Razo√°vel, poderia ser melhor.\"\n",
        "]\n",
        "\n",
        "for critica in criticas:\n",
        "    resultado = analisar_sentimento(critica)\n",
        "    print(f\"Cr√≠tica: {critica[:50]}...\")\n",
        "    print(f\"Sentimento: {resultado['sentimento']}\")\n",
        "    print(f\"Confian√ßa: {resultado['confianca']:.2%}\")\n",
        "```\n",
        "\n",
        "## üìÅ Estrutura do C√≥digo\n",
        "\n",
        "### M√≥dulos Principais\n",
        "\n",
        "1. **`preprocessamento_avancado()`**\n",
        "   - Fun√ß√£o principal de limpeza de texto\n",
        "   - Suporte a caracteres acentuados em portugu√™s\n",
        "   - Remo√ß√£o inteligente de stopwords\n",
        "\n",
        "2. **`Pipeline` de Machine Learning**\n",
        "   - Integra√ß√£o TF-IDF + Random Forest\n",
        "   - Encapsulamento completo do fluxo\n",
        "   - Facilidade de manuten√ß√£o\n",
        "\n",
        "3. **`GridSearchCV`**\n",
        "   - Busca exaustiva de melhores par√¢metros\n",
        "   - Valida√ß√£o cruzada incorporada\n",
        "   - Paraleliza√ß√£o para performance\n",
        "\n",
        "### Fluxo de Execu√ß√£o\n",
        "```\n",
        "Carregar Dados ‚Üí Pr√©-processar ‚Üí Vetorizar ‚Üí Treinar ‚Üí Otimizar ‚Üí Avaliar ‚Üí Predizer\n",
        "```\n",
        "\n",
        "## üé® Features Implementadas\n",
        "\n",
        "### ‚úÖ Corrigidas do C√≥digo Original\n",
        "- **Pr√©-processamento**: Mant√©m palavras inteiras (n√£o letras soltas)\n",
        "- **Tokeniza√ß√£o**: Usa `punkt_tab` para portugu√™s\n",
        "- **Vetoriza√ß√£o**: TF-IDF em vez de CountVectorizer simples\n",
        "- **Modelo**: Random Forest em vez de Naive Bayes b√°sico\n",
        "\n",
        "### ‚úÖ Otimiza√ß√µes Adicionais\n",
        "- Pipeline organizado com Scikit-learn\n",
        "- Otimiza√ß√£o autom√°tica de hiperpar√¢metros\n",
        "- Valida√ß√£o cruzada para avalia√ß√£o robusta\n",
        "- An√°lise detalhada de erros\n",
        "\n",
        "## üìä An√°lise de Desempenho\n",
        "\n",
        "### M√©tricas de Avalia√ß√£o\n",
        "- **Acur√°cia**: Porcentagem de classifica√ß√µes corretas\n",
        "- **Precis√£o**: Entre as classificadas como positivas, quantas realmente s√£o\n",
        "- **Recall**: Entre todas as positivas reais, quantas foram identificadas\n",
        "- **F1-Score**: M√©dia harm√¥nica entre precis√£o e recall\n",
        "\n",
        "### Matriz de Confus√£o\n",
        "```\n",
        "              Predito Negativo  Predito Positivo\n",
        "Real Negativo      TN                FP\n",
        "Real Positivo      FN                TP\n",
        "```\n",
        "\n",
        "## üîÑ Pr√≥ximas Melhorias\n",
        "\n",
        "### 1. Engenharia de Features Avan√ßada\n",
        "- [ ] Contagem de palavras positivas/negativas\n",
        "- [ ] Extra√ß√£o de emoticons e exclama√ß√µes\n",
        "- [ ] An√°lise de senten√ßas por par√°grafo\n",
        "\n",
        "### 2. Modelos Avan√ßados\n",
        "- [ ] XGBoost ou LightGBM\n",
        "- [ ] SVM com kernel n√£o-linear\n",
        "- [ ] Redes Neurais (MLP)\n",
        "\n",
        "### 3. Deep Learning\n",
        "- [ ] LSTM/GRU para contexto sequencial\n",
        "- [ ] BERTimbau (BERT em portugu√™s)\n",
        "- [ ] Fine-tuning de transformers\n",
        "\n",
        "### 4. Sistema em Produ√ß√£o\n",
        "- [ ] API REST com FastAPI\n",
        "- [ ] Sistema de cache de predi√ß√µes\n",
        "- [ ] Monitoramento de performance\n",
        "- [ ] Logs detalhados\n",
        "\n",
        "## üìù Conclus√£o\n",
        "\n",
        "Este projeto demonstra uma implementa√ß√£o completa de an√°lise de sentimentos, abordando desde o pr√©-processamento b√°sico at√© otimiza√ß√µes avan√ßadas. A arquitetura modular permite f√°cil extens√£o e adapta√ß√£o para diferentes dom√≠nios.\n",
        "\n",
        "### Principais Aprendizados\n",
        "1. **Pr√©-processamento √© crucial**: Representa√ß√£o correta dos dados afeta diretamente os resultados\n",
        "2. **TF-IDF > CountVectorizer**: Considera import√¢ncia relativa das palavras\n",
        "3. **Random Forest robusto**: Excelente para problemas de classifica√ß√£o de texto\n",
        "4. **Otimiza√ß√£o sistem√°tica**: GridSearchCV encontra automaticamente os melhores par√¢metros\n",
        "\n",
        "## üë• Contribui√ß√£o\n",
        "\n",
        "Contribui√ß√µes s√£o bem-vindas! Siga estes passos:\n",
        "\n",
        "1. Fork do reposit√≥rio\n",
        "2. Crie uma branch (`git checkout -b feature/nova-feature`)\n",
        "3. Commit suas mudan√ßas (`git commit -m 'Add nova feature'`)\n",
        "4. Push para a branch (`git push origin feature/nova-feature`)\n",
        "5. Abra um Pull Request\n",
        "\n",
        "## üìÑ Licen√ßa\n",
        "\n",
        "Este projeto est√° sob a licen√ßa MIT. Veja o arquivo [LICENSE](LICENSE) para detalhes.\n",
        "\n",
        "## üôè Agradecimentos\n",
        "\n",
        "- Dataset: [IMDB Reviews em Portugu√™s](https://www.kaggle.com/datasets)\n",
        "- Bibliotecas: Scikit-learn, NLTK, Pandas, NumPy\n",
        "- Comunidade de Data Science\n",
        "\n",
        "## üìû Contato\n",
        "\n",
        "Para d√∫vidas ou sugest√µes, entre em contato:\n",
        "\n",
        "**Desenvolvedor**: [Seu Nome]  \n",
        "**Email**: seu.email@exemplo.com  \n",
        "**LinkedIn**: [linkedin.com/in/seu-perfil](https://linkedin.com)\n",
        "\n",
        "---\n",
        "*\"Transformando texto em insights atrav√©s de dados\"* üöÄ\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **PRINCIPAIS CORRE√á√ïES APLICADAS:**\n",
        "\n",
        "1. **Corrigido erro do NLTK**: Adicionado download do `punkt_tab`\n",
        "2. **Sequ√™ncia l√≥gica**: Garantida execu√ß√£o na ordem correta\n",
        "3. **Simplifica√ß√£o**: Reduzida complexidade do GridSearchCV para execu√ß√£o mais r√°pida\n",
        "4. **Manuten√ß√£o de contexto**: Todas as vari√°veis s√£o definidas antes do uso\n",
        "\n",
        "## **PR√ìXIMOS PASSOS SUGERIDOS:**\n",
        "\n",
        "1. **Salvar o modelo treinado**:\n",
        "```python\n",
        "import joblib\n",
        "joblib.dump(grid_search, 'modelo_sentimentos.pkl')\n",
        "```\n",
        "\n",
        "2. **Criar API**:\n",
        "```python\n",
        "from fastapi import FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/analisar\")\n",
        "def analisar(critica: str):\n",
        "    texto_limpo = preprocessamento_avancado(critica)\n",
        "    predicao = grid_search.predict([texto_limpo])[0]\n",
        "    return {\"sentimento\": \"positivo\" if predicao == 1 else \"negativo\"}\n",
        "```\n",
        "\n",
        "3. **Monitoramento**:\n",
        "   - Adicionar logging\n",
        "   - Implementar tracking de performance\n",
        "   - Criar dashboard de m√©tricas\n",
        "\n",
        "O projeto est√° agora funcional e pronto para execu√ß√£o!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
